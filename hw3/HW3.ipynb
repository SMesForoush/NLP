{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2857862f",
      "metadata": {},
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        تمرین سوم درس پردازش زبان‌های طبیعی\n",
        "    </h1>\n",
        "    <h3>\n",
        "        گردآورندگان:<br/>\n",
        "        ساحل مس‌فروش، سروش تابش، درنا دهقانی\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    ما در این تمرین ترک \"تکمیل مصراع دوم با رعایت وزن شعر\" را انتخاب کردیم. در این تمرین قصد داریم به کمک مدل‌های زبانی مناسب، چند کلمه به عنوان یک مصرع از بیت را به عنوان ورودی بگیریم و چند کلمه به عنوان مصرع دوم این بیت را خروجی دهیم. در این تمرین ما از وبسایت گنجور برای جمع‌آوری داده کمک گرفتیم و از چندین قالب شعری استفاده کردیم، زیرا قافیه داشتن یا نداشتن به عنوان ورودی داده می‌شود. اگر نیاز به هم‌قافیه بودن دو مصراع بود، از قالب مثنوی نظیر شاهنماه‌ی فردوسی کمک می‌گیریم؛ و اگر نیاز به هم‌قافیه بودن دو مصراع نبود می‌توانیم از قالب قطعه استفاده کنیم. <br>\n",
        "    برای ساخت چنین سیستمی از چند مدل زبانی استفاده می‌کنیم: مدل n-gram، مدل encoder-decoder با یک شبکه LSTM به عنوان encoder، و مدلی پیرو مکانیسم توجه.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0571e",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. دوباره‌نویسی داده </h3>\n",
        "    از آنجایی که در داده‌ی گرفته شده، ابتدا و انتهای ابیات و مصراع‌ها مشخص نبود، به کمک کد زیر این داده را بازنویسی کردیم. ابتدا و انتهای ابیات با __BOB__ و __EOB__ و ابتدا و انتهای مصراع‌ها با __BOM__ و __EOM__ مشخص گردید.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ab86f6b",
      "metadata": {
        "id": "9ab86f6b"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3251a5c1",
      "metadata": {
        "id": "3251a5c1"
      },
      "outputs": [],
      "source": [
        "path_dir = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed4fe8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4fe8e2",
        "outputId": "de1a16d9-f71f-4d5e-a483-65c9b2c5bc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: hazm in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Requirement already satisfied: transformers in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: requests in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (4.63.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: torch in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8425cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8425cfb",
        "outputId": "1d76e685-8a26-4380-947e-7d7f6315b89c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1902c48a",
      "metadata": {
        "id": "1902c48a"
      },
      "outputs": [],
      "source": [
        "class Cleaner:\n",
        "    def __init__(self):\n",
        "        self.normalizer = Normalizer()\n",
        "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
        "        self.file_range = None\n",
        "\n",
        "    def add_statement_mesras(self, file_out='f_beyt.txt'):\n",
        "        start_mesra = '[BOM]'\n",
        "        end_mesra = ''\n",
        "        start_beyt = ''\n",
        "        end_beyt = '[EOS]' + os.linesep\n",
        "        beyt_file = ''\n",
        "        with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "            lines = fp.readlines()\n",
        "            for i in range(0, len(lines) - 1, 2):\n",
        "                mesra1 = start_mesra + lines[i].strip() + end_mesra\n",
        "                mesra2 = start_mesra + lines[i + 1].strip() + end_mesra\n",
        "                b = start_beyt + mesra1.strip() + ' ' + mesra2.strip() + end_beyt\n",
        "                beyt_file += b\n",
        "                \n",
        "        with open(file_out, 'w', encoding=\"utf-8\") as fp:\n",
        "            fp.write(beyt_file)\n",
        "\n",
        "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  \n",
        "        file_range = range(*file_range) if file_range is not None else None \n",
        "        file_names = glob.glob(file_patterns)\n",
        "        mesra_collection = []\n",
        "        for file_name in file_names:\n",
        "            if normalize:\n",
        "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
        "            else:\n",
        "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
        "        return mesra_collection\n",
        "    \n",
        "\n",
        "    def read_documents(self):\n",
        "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
        "        cleaned = self.clean_data(poems)\n",
        "        return ' \\n '.join(cleaned)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f754d9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel\n",
        ")\n",
        "\n",
        "class MesraGenerator:\n",
        "    def __init__(self, path):\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.train_path = path\n",
        "        self.cleaner = Cleaner()\n",
        "\n",
        "    def read_data(self):\n",
        "        train_dataset = TextDataset(\n",
        "            tokenizer=tokenizer, file_path=self.train_path, block_size=256)\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "        return train_dataset, data_collator\n",
        "\n",
        "    def read_model(self, model_type='bolbolzaban/gpt2-persian'):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "\n",
        "    def freeze_lower_layers(self):\n",
        "        for param in self.model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in (\n",
        "            self.model.base_model.h[23].parameters() or self.model.base_model.h[22].parameters()\n",
        "        ):\n",
        "            param.requires_grad = True\n",
        "            \n",
        "    def fine_tune_model(self):\n",
        "        training_args = TrainingArguments(\n",
        "        output_dir=\"./model\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "        # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "        per_device_train_batch_size=12,\n",
        "        per_device_eval_batch_size=12,\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        warmup_steps=500)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_texts,\n",
        "            eval_dataset=val_texts,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "    def save_model(self, dir=None):\n",
        "        self.trainer.save_model(output_dir=dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e9b0bd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b0bd15",
        "outputId": "0116c322-0947-4283-8522-2682d531889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99217\n"
          ]
        }
      ],
      "source": [
        "path=f\"{path_dir}/ferdousi.txt\"\n",
        "cleaner = Cleaner()\n",
        "cleaner.add_statement_mesras()\n",
        "mesras = cleaner.read_poems(file_patterns=path, file_range=None, normalize=False)\n",
        "print(len(mesras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5e8521",
      "metadata": {
        "id": "dd5e8521",
        "outputId": "3b0a2062-3405-4cb2-e56f-f513e1d75b9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 452M/452M [03:31<00:00, 2.23MB/s]   \n",
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['ما',\n",
              " 'در',\n",
              " 'هوش',\n",
              " '[ZWNJ]',\n",
              " 'واره',\n",
              " 'معتقدیم',\n",
              " 'با',\n",
              " 'انتقال',\n",
              " 'صحیح',\n",
              " 'دانش',\n",
              " 'و',\n",
              " 'آ',\n",
              " '##گاهی',\n",
              " '،',\n",
              " 'همه',\n",
              " 'افراد',\n",
              " 'میتوانند',\n",
              " 'از',\n",
              " 'ابزارهای',\n",
              " 'هوشمند',\n",
              " 'استفاده',\n",
              " 'کنند',\n",
              " '.',\n",
              " 'شعار',\n",
              " 'ما',\n",
              " 'هوش',\n",
              " 'مصنوعی',\n",
              " 'برای',\n",
              " 'همه',\n",
              " 'است',\n",
              " '.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
        "\n",
        "# v3.0\n",
        "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
        "model = AutoModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "text = \"ما در هوش‌واره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\n",
        "tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7dd04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7dd04d",
        "outputId": "da16e9d2-bad0-4973-8ac7-f7117de1fbb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Freezing the lower layers increases the training speed and reduces the memory requirement.\n",
        "# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n",
        "# or unfreeze as many layers that you can affort training with a reasonable batchsize.\n",
        "\n",
        "\n",
        "\n",
        "# load model\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "\n",
        "# freeze lower layers and only train top layers\n",
        "freeze_lower_layers()\n",
        "\n",
        "# load dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "train_dataset, test_dataset, data_collator = load_dataset(\n",
        "    \"ferdousi.txt\", \"test.txt\", tokenizer\n",
        ")\n",
        "print(type(train_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "gXpjLVYYaXjQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpjLVYYaXjQ",
        "outputId": "bdc3fedc-9e85-40c5-af47-5ebc13776b90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'freeze_lower_layers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbolbolzaban/gpt2-persian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbolbolzaban/gpt2-persian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=3'>4</a>\u001b[0m freeze_lower_layers()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=4'>5</a>\u001b[0m generator \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m'\u001b[39m, model, tokenizer\u001b[39m=\u001b[39mtokenizer, config\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m256\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=5'>6</a>\u001b[0m sample \u001b[39m=\u001b[39m generator(\u001b[39m'\u001b[39m\u001b[39mدر یک اتفاق شگفت انگیز، پژوهشگران\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'freeze_lower_layers' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "\n",
        "freeze_lower_layers()\n",
        "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
        "sample = generator('در یک اتفاق شگفت انگیز، پژوهشگران')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OyvnTBDCdH3s",
      "metadata": {
        "id": "OyvnTBDCdH3s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aLCtmbDhbkPB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCtmbDhbkPB",
        "outputId": "c1948abf-ad6d-4c41-9509-b7f7590e7016"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000015?line=0'>1</a>\u001b[0m generator(\u001b[39m'\u001b[39m\u001b[39m توانا بود\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
          ]
        }
      ],
      "source": [
        "generator(' توانا بود')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9sdNgE4abLmi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sdNgE4abLmi",
        "outputId": "c9e604cb-44e7-414e-bed5-56a732f1635c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'در یک اتفاق شگفت انگیز، پژوهشگران آمریکایی ، موفق به کشف آن شدند.'}]\n"
          ]
        }
      ],
      "source": [
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d906e9",
      "metadata": {
        "id": "64d906e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts = train_test_split(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156a8c43",
      "metadata": {
        "id": "156a8c43",
        "outputId": "17e5440f-9532-4edc-fb1f-3c69bb594aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03004f61",
      "metadata": {
        "id": "03004f61",
        "outputId": "5809efaf-8475-4ba9-f79e-63aae641d416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(val_texts))\n",
        "print(type(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936486e8",
      "metadata": {
        "id": "936486e8",
        "outputId": "149b9b9c-b3ef-449b-9d04-bc0112636736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1895\n",
            "632\n",
            "2527\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9ff25dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "9ff25dd9",
        "outputId": "28249912-10a7-41a0-9f22-c3b43be8e8b5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TrainingArguments' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=1'>2</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=3'>4</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=4'>5</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=5'>6</a>\u001b[0m     \u001b[39m# Set the batch size to a maximum value that could fit into GPU memory,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=6'>7</a>\u001b[0m     \u001b[39m# for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=7'>8</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=8'>9</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=9'>10</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=10'>11</a>\u001b[0m     save_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=11'>12</a>\u001b[0m     warmup_steps\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=14'>15</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=16'>17</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=19'>20</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_texts,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
          ]
        }
      ],
      "source": [
        "# train\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_texts,\n",
        "    eval_dataset=val_texts,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NV73xepfgy90",
      "metadata": {
        "id": "NV73xepfgy90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
