{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xI1r8CqCu7Fh",
    "outputId": "850ea181-141c-4d1b-ca57-ea5d37b1232e"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 23:04:19 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| N/A   62C    P0    N/A /  N/A |    710MiB /  2002MiB |     15%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1322      G   /usr/lib/xorg/Xorg                104MiB |\r\n",
      "|    0   N/A  N/A      2644      G   /usr/lib/xorg/Xorg                367MiB |\r\n",
      "|    0   N/A  N/A      2823      G   /usr/bin/gnome-shell               56MiB |\r\n",
      "|    0   N/A  N/A     12966      G   ...AAAAAAAAA= --shared-files       28MiB |\r\n",
      "|    0   N/A  N/A    277211      G   ...AAAAAAAA== --shared-files       26MiB |\r\n",
      "|    0   N/A  N/A    277237      G   ...AAAAAAAAA= --shared-files       18MiB |\r\n",
      "|    0   N/A  N/A    277866      G   ...f_3329.log --shared-files       13MiB |\r\n",
      "|    0   N/A  N/A    329575      G   ...748528155973965074,131072       82MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -q wandb\n",
    "! git clone \"https://github.com/amnghd/Persian_poems_corpus.git\"\n",
    "! mkdir \"corpus\"\n",
    "! cp \"Persian_poems_corpus/normalized/ferdousi_norm.txt\" \"Persian_poems_corpus/normalized/hafez_norm.txt\" \"Persian_poems_corpus/normalized/moulavi_norm.txt\"\n",
    "\"./corpus/\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GY8_SYkxrHx1",
    "outputId": "2e38c4cb-968b-4436-8a22-bfa1f805e2c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'Persian_poems_corpus' already exists and is not an empty directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soroush/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.is_available())\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf4OM-5VnNsA",
    "outputId": "2654bcd1-3f28-4efa-c236-3e718cf7ea2e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQgc5YEUq1T7",
    "outputId": "442e8cac-e0d6-4740-eee5-9409ac2d2856"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass\n",
    "wandb_active = False\n",
    "run_name = 'all_poem_train'\n",
    "\n",
    "if wandb_active:\n",
    "    wandb.init(project=\"ferdousi-generator\", name=run_name)\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = Config()\n",
    "config.max_epochs = 10\n",
    "config.batch_size = 256\n",
    "config.embedding_size = 512\n",
    "config.lstm_num_layers = 3\n",
    "config.lstm_hidden_size = 512\n",
    "config.sequence_length = 10\n",
    "config.log_interval = 10\n",
    "config.learning_rate = 0.001\n",
    "config.vocab_size = 38590\n",
    "config.lstm_dropout = 0.2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hhC7SjF5nNsE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "id": "q9sBywu_nNsF"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset, config, device=torch.device('cpu')):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = config.embedding_size\n",
    "        self.lstm_hidden_size = config.lstm_hidden_size\n",
    "        self.lstm_dropout = 0.2\n",
    "        self.embedding_dim = config.embedding_size\n",
    "        self.num_layers = config.lstm_num_layers\n",
    "        self.device = device\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.lstm_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, self.vocab_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "38590"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            device=torch.device('cpu'),\n",
    "            poet='ferdousi',\n",
    "            corpus_dir='./Persian_poems_corpus/normalized',\n",
    "            vocab_path='./vocabulary.txt'\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.vocab_path = vocab_path\n",
    "\n",
    "        self.words_by_poet = self.load_words(corpus_dir)\n",
    "        self.vocabulary = self.load_vocabulary()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.poet = poet\n",
    "\n",
    "    def preprocess_lines(self, lines, mask_key):\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = filter(lambda line: len(line) > 0, lines)\n",
    "        lines = map(lambda line: line.replace('\\n',''), lines)\n",
    "        lines = map(lambda line: line.replace('\\t',''), lines)\n",
    "        lines = map(lambda line: line.replace('\\r',''), lines)\n",
    "        lines = map(\n",
    "            lambda index_line:\n",
    "            f'[BOM_{mask_key}] ' + index_line[1] + ' [EOS]' if index_line[0] % 2 == 1\n",
    "            else f'[BOM_{mask_key}] ' + index_line[1],\n",
    "            enumerate(lines)\n",
    "        )\n",
    "        words = itertools.chain.from_iterable(map(lambda line: line.split(' '), lines))\n",
    "        words = filter(lambda word: len(word) > 0, words)\n",
    "        words = list(words)\n",
    "        return words\n",
    "\n",
    "    def load_words(self, corpus_dir):\n",
    "        words_by_poet = {}\n",
    "        for filename in os.listdir(corpus_dir):\n",
    "            with open(os.path.join(corpus_dir, filename)) as f:\n",
    "                poet_name = filename.split('_')[0]\n",
    "                lines = f.readlines()\n",
    "                words_by_poet[poet_name] = self.preprocess_lines(lines, poet_name)\n",
    "        return words_by_poet\n",
    "\n",
    "    def load_vocabulary(self):\n",
    "        with open(self.vocab_path) as f:\n",
    "            vocabulary = f.readlines()\n",
    "        vocabulary = [word.strip() for word in vocabulary]\n",
    "        return vocabulary\n",
    "\n",
    "    @property\n",
    "    def all_poets(self):\n",
    "        return self.words_by_poet.keys()\n",
    "\n",
    "    @property\n",
    "    def poet(self):\n",
    "        return self._poet\n",
    "\n",
    "    @poet.setter\n",
    "    def poet(self, poet):\n",
    "        self._poet = poet\n",
    "        if poet == 'all':\n",
    "            self.words = list(itertools.chain.from_iterable(self.words_by_poet.values()))\n",
    "        else:\n",
    "            self.words = self.words_by_poet[poet]\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.config.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensors = (\n",
    "            torch.tensor(self.words_indexes[index:index + self.config.sequence_length]).to(self.device),\n",
    "            torch.tensor(self.words_indexes[index + 1:index + self.config.sequence_length + 1]).to(self.device),\n",
    "        )\n",
    "        return tensors\n",
    "\n",
    "dataset = PoemDataset(config, device=torch.device('cpu'), poet='ferdousi', corpus_dir='../data/poems', vocab_path='../data/vocabulary.txt')\n",
    "len(dataset.vocabulary)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KZ_aYCW6nNsG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this cell to generate new vocabulary\n",
    "\n",
    "all_words = list(itertools.chain.from_iterable(dataset.words_by_poet.values()))\n",
    "word_counts = Counter(all_words)\n",
    "vocab = sorted(list(word_counts))\n",
    "with open('../data/vocabulary.txt', 'w') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def train(dataset, model, config, checkpoint_path='../data/checkpoints'):\n",
    "    if wandb_active:\n",
    "        wandb.watch(model)\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    print({'batch_count': len(dataloader), 'epoch_count': config.max_epochs})\n",
    "    for epoch in range(config.max_epochs):\n",
    "        state_h, state_c = model.init_state(config.sequence_length)\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'epoch': epoch, 'batch': batch, 'loss': loss.item()})\n",
    "            if wandb_active and batch % config.log_interval == 0:\n",
    "                wandb.log({\"loss\": loss})\n",
    "        try:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, os.path.join(checkpoint_path, f'model_{run_name}_checkpoint_{time.time()}.pt'))\n",
    "        except:\n",
    "            pass"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nUkrBNA5nNsH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, max_predict_length=12):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    i = 0\n",
    "    while words[-1] != '[EOS]' and i < max_predict_length:\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        i+=1\n",
    "    return words"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P5VE6ck8nNsH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    0,  8415, 28930, 13192, 11519, 32103, 13311,     0, 36098,  6212]), tensor([ 8415, 28930, 13192, 11519, 32103, 13311,     0, 36098,  6212,  3341]))\n",
      "(tensor([ 8415, 28930, 13192, 11519, 32103, 13311,     0, 36098,  6212,  3341]), tensor([28930, 13192, 11519, 32103, 13311,     0, 36098,  6212,  3341,  6732]))\n",
      "(tensor([28930, 13192, 11519, 32103, 13311,     0, 36098,  6212,  3341,  6732]), tensor([13192, 11519, 32103, 13311,     0, 36098,  6212,  3341,  6732,     3]))\n",
      "(tensor([13192, 11519, 32103, 13311,     0, 36098,  6212,  3341,  6732,     3]), tensor([11519, 32103, 13311,     0, 36098,  6212,  3341,  6732,     3,     0]))\n",
      "(tensor([11519, 32103, 13311,     0, 36098,  6212,  3341,  6732,     3,     0]), tensor([32103, 13311,     0, 36098,  6212,  3341,  6732,     3,     0, 13192]))\n",
      "(tensor([32103, 13311,     0, 36098,  6212,  3341,  6732,     3,     0, 13192]), tensor([13311,     0, 36098,  6212,  3341,  6732,     3,     0, 13192, 28930]))\n",
      "(tensor([13311,     0, 36098,  6212,  3341,  6732,     3,     0, 13192, 28930]), tensor([    0, 36098,  6212,  3341,  6732,     3,     0, 13192, 28930, 32103]))\n",
      "(tensor([    0, 36098,  6212,  3341,  6732,     3,     0, 13192, 28930, 32103]), tensor([36098,  6212,  3341,  6732,     3,     0, 13192, 28930, 32103, 13192]))\n",
      "(tensor([36098,  6212,  3341,  6732,     3,     0, 13192, 28930, 32103, 13192]), tensor([ 6212,  3341,  6732,     3,     0, 13192, 28930, 32103, 13192, 11600]))\n",
      "(tensor([ 6212,  3341,  6732,     3,     0, 13192, 28930, 32103, 13192, 11600]), tensor([ 3341,  6732,     3,     0, 13192, 28930, 32103, 13192, 11600,     0]))\n"
     ]
    }
   ],
   "source": [
    "dataset = PoemDataset(config, device=torch.device('cpu'), poet='ferdousi', corpus_dir='../data/poems', vocab_path='../data/vocabulary.txt')\n",
    "for i in range(10):\n",
    "    print(dataset[i])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aNDW-n0TnNsJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_count': 2803, 'epoch_count': 10}\n",
      "{'epoch': 0, 'batch': 0, 'loss': 10.565759658813477}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_329939/3331772141.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_329939/639510533.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(dataset, model, config, checkpoint_path)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0my_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mstate_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_c\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mstate_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_c\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m             \u001B[0mstate_h\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstate_h\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    960\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    961\u001B[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0;32m--> 962\u001B[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001B[0m\u001B[1;32m    963\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    964\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[1;32m   2466\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2467\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2468\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnll_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2469\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2470\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlog_softmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1603\u001B[0m         \u001B[0mdim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_softmax_dim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'log_softmax'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_stacklevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1604\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1605\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1606\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1607\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(dataset, config, device)\n",
    "train(dataset, model, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(dataset, config, device)\n",
    "chechkpoint = torch.load('/content/drive/MyDrive/NLP Class/checkpoints/model_checkpoint_1654448961.97679.pt',\n",
    "                         map_location=torch.device('cpu'))\n",
    "print(chechkpoint['epoch'])\n",
    "model.load_state_dict(chechkpoint['model_state_dict'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vQI80qiTLb8",
    "outputId": "309bffc5-18d2-4b22-ed24-5810d6421102"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOM_ferdousi]\n",
      "توانا\n",
      "بود\n",
      "هر\n",
      "که\n",
      "جانند\n",
      "مستوی\n",
      "سنگیان\n",
      "پنداشت\n",
      "مالامالیم\n",
      "انگاشتیم\n",
      "مقراضی\n",
      "خضرم\n",
      "چفسیده\n",
      "التقا\n",
      "شهرهار\n",
      "خاکا\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n'.join(predict(dataset, model, text='[BOM_ferdousi] توانا بود هر که')))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rAGcmLZ2nNsJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e1ce9911-7781-4703-af4b-56e7894f9bc0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.word_to_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27a225e506849d1bdf80008d630248f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▆▅▅▆▅▃▃▄▃▃▃▃▂▃▃▃▃▃▃▃▃▂▁▃▃▂▂▂▂▂▂▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>4.09747</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">dark-butterfly-5</strong>: <a href=\"https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o\" target=\"_blank\">https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220604_214452-38pzbi2o/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "referenced_widgets": [
      "a27a225e506849d1bdf80008d630248f"
     ]
    },
    "id": "k3OKozxCnNsK",
    "outputId": "0e015429-bfa2-4342-91ed-e652878e5b71"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "NLP-HW3-LSTM.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}