{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2857862f",
      "metadata": {},
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        ØªÙ…Ø±ÛŒÙ† Ø³ÙˆÙ… Ø¯Ø±Ø³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ\n",
        "    </h1>\n",
        "    <h3>\n",
        "        Ú¯Ø±Ø¯Ø¢ÙˆØ±Ù†Ø¯Ú¯Ø§Ù†:<br/>\n",
        "        Ø³Ø§Ø­Ù„ Ù…Ø³â€ŒÙØ±ÙˆØ´ØŒ Ø³Ø±ÙˆØ´ ØªØ§Ø¨Ø´ØŒ Ø¯Ø±Ù†Ø§ Ø¯Ù‡Ù‚Ø§Ù†ÛŒ\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† ØªØ±Ú© \"ØªÚ©Ù…ÛŒÙ„ Ù…ØµØ±Ø§Ø¹ Ø¯ÙˆÙ… Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª ÙˆØ²Ù† Ø´Ø¹Ø±\" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ú©Ù…Ú© Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ù…Ù†Ø§Ø³Ø¨ØŒ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…ØµØ±Ø¹ Ø§Ø² Ø¨ÛŒØª Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ú¯ÛŒØ±ÛŒÙ… Ùˆ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ØµØ±Ø¹ Ø¯ÙˆÙ… Ø§ÛŒÙ† Ø¨ÛŒØª Ø±Ø§ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ù‡ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ù…Ø§ Ø§Ø² ÙˆØ¨Ø³Ø§ÛŒØª Ú¯Ù†Ø¬ÙˆØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ú©Ù…Ú© Ú¯Ø±ÙØªÛŒÙ… Ùˆ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† Ù‚Ø§Ù„Ø¨ Ø´Ø¹Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø²ÛŒØ±Ø§ Ù‚Ø§ÙÛŒÙ‡ Ø¯Ø§Ø´ØªÙ† ÛŒØ§ Ù†Ø¯Ø§Ø´ØªÙ† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù…â€ŒÙ‚Ø§ÙÛŒÙ‡ Ø¨ÙˆØ¯Ù† Ø¯Ùˆ Ù…ØµØ±Ø§Ø¹ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù‚Ø§Ù„Ø¨ Ù…Ø«Ù†ÙˆÛŒ Ù†Ø¸ÛŒØ± Ø´Ø§Ù‡Ù†Ù…Ø§Ù‡â€ŒÛŒ ÙØ±Ø¯ÙˆØ³ÛŒ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…Ø› Ùˆ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù…â€ŒÙ‚Ø§ÙÛŒÙ‡ Ø¨ÙˆØ¯Ù† Ø¯Ùˆ Ù…ØµØ±Ø§Ø¹ Ù†Ø¨ÙˆØ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ù‚Ø§Ù„Ø¨ Ù‚Ø·Ø¹Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. <br>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ú†Ù†ÛŒÙ† Ø³ÛŒØ³ØªÙ…ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…: Ù…Ø¯Ù„ n-gramØŒ Ù…Ø¯Ù„ encoder-decoder Ø¨Ø§ ÛŒÚ© Ø´Ø¨Ú©Ù‡ LSTM Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† encoderØŒ Ùˆ Ù…Ø¯Ù„ÛŒ Ù¾ÛŒØ±Ùˆ Ù…Ú©Ø§Ù†ÛŒØ³Ù… ØªÙˆØ¬Ù‡.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0571e",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. Ø¯ÙˆØ¨Ø§Ø±Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø¯Ø§Ø¯Ù‡ </h3>\n",
        "    Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø§Ø¨ÛŒØ§Øª Ùˆ Ù…ØµØ±Ø§Ø¹â€ŒÙ‡Ø§ Ù…Ø´Ø®Øµ Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ú©Ù…Ú© Ú©Ø¯ Ø²ÛŒØ± Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ø±Ø¯ÛŒÙ…. Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø§Ø¨ÛŒØ§Øª Ø¨Ø§ __BOB__ Ùˆ __EOB__ Ùˆ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØµØ±Ø§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ __BOM__ Ùˆ __EOM__ Ù…Ø´Ø®Øµ Ú¯Ø±Ø¯ÛŒØ¯.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ab86f6b",
      "metadata": {
        "id": "9ab86f6b"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b238822b",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_path = \"./data\"\n",
        "path = os.path.join(base_path, 'ferdousi_norm.txt')\n",
        "\n",
        "beyt_file = ''\n",
        "with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "    lines = fp.readlines()\n",
        "    for i in range(0, len(lines) - 1, 2):\n",
        "        mesra1 = '[BOM]' + lines[i].strip()\n",
        "        mesra2 = '[BOM]' + lines[i + 1].strip()\n",
        "        b = mesra1.strip() + ' ' + mesra2.strip() + '[EOS]\\n'\n",
        "        beyt_file += b\n",
        "        \n",
        "with open('f_beyt.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "    fp.write(beyt_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3251a5c1",
      "metadata": {
        "id": "3251a5c1"
      },
      "outputs": [],
      "source": [
        "path_dir = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed4fe8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4fe8e2",
        "outputId": "de1a16d9-f71f-4d5e-a483-65c9b2c5bc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: hazm in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Requirement already satisfied: transformers in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: requests in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (4.63.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: torch in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8425cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8425cfb",
        "outputId": "1d76e685-8a26-4380-947e-7d7f6315b89c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1902c48a",
      "metadata": {
        "id": "1902c48a"
      },
      "outputs": [],
      "source": [
        "class Cleaner:\n",
        "    def __init__(self):\n",
        "        self.normalizer = Normalizer()\n",
        "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
        "        self.file_range = None\n",
        "\n",
        "    def add_statement_mesras(self, file_out='f_beyt.txt'):\n",
        "        start_mesra = '[BOM]'\n",
        "        end_mesra = ''\n",
        "        start_beyt = ''\n",
        "        end_beyt = '[EOS]' + os.linesep\n",
        "        beyt_file = ''\n",
        "        with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "            lines = fp.readlines()\n",
        "            for i in range(0, len(lines) - 1, 2):\n",
        "                mesra1 = start_mesra + lines[i].strip() + end_mesra\n",
        "                mesra2 = start_mesra + lines[i + 1].strip() + end_mesra\n",
        "                b = start_beyt + mesra1.strip() + ' ' + mesra2.strip() + end_beyt\n",
        "                beyt_file += b\n",
        "                \n",
        "        with open(file_out, 'w', encoding=\"utf-8\") as fp:\n",
        "            fp.write(beyt_file)\n",
        "\n",
        "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  \n",
        "        file_range = range(*file_range) if file_range is not None else None \n",
        "        file_names = glob.glob(file_patterns)\n",
        "        mesra_collection = []\n",
        "        for file_name in file_names:\n",
        "            if normalize:\n",
        "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
        "            else:\n",
        "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
        "        return mesra_collection\n",
        "    \n",
        "\n",
        "    def read_documents(self):\n",
        "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
        "        cleaned = self.clean_data(poems)\n",
        "        return ' \\n '.join(cleaned)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e9b0bd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b0bd15",
        "outputId": "0116c322-0947-4283-8522-2682d531889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99217\n"
          ]
        }
      ],
      "source": [
        "path=f\"{path_dir}/ferdousi.txt\"\n",
        "cleaner = Cleaner()\n",
        "cleaner.add_statement_mesras()\n",
        "mesras = cleaner.read_poems(file_patterns=path, file_range=None, normalize=False)\n",
        "print(len(mesras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5e8521",
      "metadata": {
        "id": "dd5e8521",
        "outputId": "3b0a2062-3405-4cb2-e56f-f513e1d75b9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 452M/452M [03:31<00:00, 2.23MB/s]   \n",
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Ù…Ø§',\n",
              " 'Ø¯Ø±',\n",
              " 'Ù‡ÙˆØ´',\n",
              " '[ZWNJ]',\n",
              " 'ÙˆØ§Ø±Ù‡',\n",
              " 'Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ…',\n",
              " 'Ø¨Ø§',\n",
              " 'Ø§Ù†ØªÙ‚Ø§Ù„',\n",
              " 'ØµØ­ÛŒØ­',\n",
              " 'Ø¯Ø§Ù†Ø´',\n",
              " 'Ùˆ',\n",
              " 'Ø¢',\n",
              " '##Ú¯Ø§Ù‡ÛŒ',\n",
              " 'ØŒ',\n",
              " 'Ù‡Ù…Ù‡',\n",
              " 'Ø§ÙØ±Ø§Ø¯',\n",
              " 'Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯',\n",
              " 'Ø§Ø²',\n",
              " 'Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ',\n",
              " 'Ù‡ÙˆØ´Ù…Ù†Ø¯',\n",
              " 'Ø§Ø³ØªÙØ§Ø¯Ù‡',\n",
              " 'Ú©Ù†Ù†Ø¯',\n",
              " '.',\n",
              " 'Ø´Ø¹Ø§Ø±',\n",
              " 'Ù…Ø§',\n",
              " 'Ù‡ÙˆØ´',\n",
              " 'Ù…ØµÙ†ÙˆØ¹ÛŒ',\n",
              " 'Ø¨Ø±Ø§ÛŒ',\n",
              " 'Ù‡Ù…Ù‡',\n",
              " 'Ø§Ø³Øª',\n",
              " '.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
        "\n",
        "# v3.0\n",
        "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
        "model = AutoModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "text = \"Ù…Ø§ Ø¯Ø± Ù‡ÙˆØ´â€ŒÙˆØ§Ø±Ù‡ Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ… Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ ØµØ­ÛŒØ­ Ø¯Ø§Ù†Ø´ Ùˆ Ø¢Ú¯Ø§Ù‡ÛŒØŒ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯. Ø´Ø¹Ø§Ø± Ù…Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø³Øª.\"\n",
        "tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7dd04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7dd04d",
        "outputId": "da16e9d2-bad0-4973-8ac7-f7117de1fbb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel\n",
        ")\n",
        "\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer, file_path=train_path, block_size=256\n",
        "    )\n",
        "\n",
        "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "\n",
        "# Freezing the lower layers increases the training speed and reduces the memory requirement.\n",
        "# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n",
        "# or unfreeze as many layers that you can affort training with a reasonable batchsize.\n",
        "def freeze_lower_layers():\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in (\n",
        "        model.base_model.h[23].parameters() or model.base_model.h[22].parameters()\n",
        "    ):\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "# load model\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "\n",
        "# freeze lower layers and only train top layers\n",
        "freeze_lower_layers()\n",
        "\n",
        "# load dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "train_dataset, test_dataset, data_collator = load_dataset(\n",
        "    \"ferdousi.txt\", \"test.txt\", tokenizer\n",
        ")\n",
        "print(type(train_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gXpjLVYYaXjQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpjLVYYaXjQ",
        "outputId": "bdc3fedc-9e85-40c5-af47-5ebc13776b90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/5150b65e3adedfb2ee4dbf8cf3f2678d10fa7eb47ea60a4e45d91ad65b2ad9ac.cba88ff49c3393ce942262df421f241abe04b1faf9dfa51b28ff9c74dbd0894e\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5093edd49280bca5e77d3a2b65210aed0624d625f8e662ba4b7bae9ac368a26a.a4f7a3e25dc07fe97c6be742bc492f8d771c52b091b133266088e9e55a40343f\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/f2610d634888d7f6e69ab99f04e9013fbe291b9dfb3e3c0907e1c36215faf094.d4d3bb8ba869519766ba55c28fb7ac26fa04d3dd52b286eb8c77adde589387ed\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/917016d11d789feaa0782d8c93b7e65adfb036276c3c8f44bb354733033ee3ef.5d97eae2282178dfbf0a4d70367a49303af74d87d1b86eb8da118577fa61246f\n",
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"./\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c15cc7211209da41b45823a6402d55435aca4a83aaf7add6a53a2b59e58a67a2.456cf2e006b146f734c3c33edfcf7d564ae3157b9e8be51da7158c2b8cd05463\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at bolbolzaban/gpt2-persian.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "freeze_lower_layers()\n",
        "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
        "sample = generator('Ø¯Ø± ÛŒÚ© Ø§ØªÙØ§Ù‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ²ØŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù†')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OyvnTBDCdH3s",
      "metadata": {
        "id": "OyvnTBDCdH3s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aLCtmbDhbkPB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCtmbDhbkPB",
        "outputId": "c1948abf-ad6d-4c41-9509-b7f7590e7016"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000015?line=0'>1</a>\u001b[0m generator(\u001b[39m'\u001b[39m\u001b[39m ØªÙˆØ§Ù†Ø§ Ø¨ÙˆØ¯\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
          ]
        }
      ],
      "source": [
        "generator(' ØªÙˆØ§Ù†Ø§ Ø¨ÙˆØ¯')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9sdNgE4abLmi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sdNgE4abLmi",
        "outputId": "c9e604cb-44e7-414e-bed5-56a732f1635c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Ø¯Ø± ÛŒÚ© Ø§ØªÙØ§Ù‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ²ØŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ ØŒ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø´Ù Ø¢Ù† Ø´Ø¯Ù†Ø¯.'}]\n"
          ]
        }
      ],
      "source": [
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d906e9",
      "metadata": {
        "id": "64d906e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts = train_test_split(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156a8c43",
      "metadata": {
        "id": "156a8c43",
        "outputId": "17e5440f-9532-4edc-fb1f-3c69bb594aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03004f61",
      "metadata": {
        "id": "03004f61",
        "outputId": "5809efaf-8475-4ba9-f79e-63aae641d416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(val_texts))\n",
        "print(type(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936486e8",
      "metadata": {
        "id": "936486e8",
        "outputId": "149b9b9c-b3ef-449b-9d04-bc0112636736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1895\n",
            "632\n",
            "2527\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff25dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "9ff25dd9",
        "outputId": "28249912-10a7-41a0-9f22-c3b43be8e8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-17eb82a5a28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 14.76 GiB total capacity; 13.22 GiB already allocated; 93.75 MiB free; 13.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# train\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_texts,\n",
        "    eval_dataset=val_texts,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NV73xepfgy90",
      "metadata": {
        "id": "NV73xepfgy90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
