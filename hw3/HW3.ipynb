{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2857862f",
      "metadata": {},
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        تمرین سوم درس پردازش زبان‌های طبیعی\n",
        "    </h1>\n",
        "    <h3>\n",
        "        گردآورندگان:<br/>\n",
        "        ساحل مس‌فروش، سروش تابش، درنا دهقانی\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    ما در این تمرین ترک \"تکمیل مصراع دوم با رعایت وزن شعر\" را انتخاب کردیم. در این تمرین قصد داریم به کمک مدل‌های زبانی مناسب، چند کلمه به عنوان یک مصرع از بیت را به عنوان ورودی بگیریم و چند کلمه به عنوان مصرع دوم این بیت را خروجی دهیم. در این تمرین ما از وبسایت گنجور برای جمع‌آوری داده کمک گرفتیم و از چندین قالب شعری استفاده کردیم، زیرا قافیه داشتن یا نداشتن به عنوان ورودی داده می‌شود. اگر نیاز به هم‌قافیه بودن دو مصراع بود، از قالب مثنوی نظیر شاهنماه‌ی فردوسی کمک می‌گیریم؛ و اگر نیاز به هم‌قافیه بودن دو مصراع نبود می‌توانیم از قالب قطعه استفاده کنیم. <br>\n",
        "    برای ساخت چنین سیستمی از چند مدل زبانی استفاده می‌کنیم: مدل n-gram، مدل encoder-decoder با یک شبکه LSTM به عنوان encoder، و مدلی پیرو مکانیسم توجه.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0571e",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. دوباره‌نویسی داده </h3>\n",
        "    از آنجایی که در داده‌ی گرفته شده، ابتدا و انتهای ابیات و مصراع‌ها مشخص نبود، به کمک کد زیر این داده را بازنویسی کردیم. ابتدا و انتهای ابیات با __BOB__ و __EOB__ و ابتدا و انتهای مصراع‌ها با __BOM__ و __EOM__ مشخص گردید.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ab86f6b",
      "metadata": {
        "id": "9ab86f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel\n",
        ")\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import os\n",
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed4fe8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4fe8e2",
        "outputId": "de1a16d9-f71f-4d5e-a483-65c9b2c5bc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: hazm in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Requirement already satisfied: transformers in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: requests in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (4.63.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: torch in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1902c48a",
      "metadata": {
        "id": "1902c48a"
      },
      "outputs": [],
      "source": [
        "class Cleaner:\n",
        "    def __init__(self):\n",
        "        self.normalizer = Normalizer()\n",
        "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
        "        self.file_range = None\n",
        "\n",
        "    def add_statement_mesras(self, file_in= \"./data/ferdousi_norm.txt\", file_out='./data/f_beyt.txt'):\n",
        "        start_mesra = '[BOM]'\n",
        "        end_mesra = ''\n",
        "        start_beyt = ''\n",
        "        end_beyt = '[EOS]' + os.linesep\n",
        "        beyt_file = ''\n",
        "        with open(file_in, 'r', encoding=\"utf-8\") as fp:\n",
        "            lines = fp.readlines()\n",
        "            for i in range(0, len(lines) - 1, 2):\n",
        "                mesra1 = start_mesra + lines[i].strip() + end_mesra\n",
        "                mesra2 = start_mesra + lines[i + 1].strip() + end_mesra\n",
        "                b = start_beyt + mesra1.strip() + ' ' + mesra2.strip() + end_beyt\n",
        "                beyt_file += b\n",
        "                \n",
        "        with open(file_out, 'w', encoding=\"utf-8\") as fp:\n",
        "            fp.write(beyt_file)\n",
        "\n",
        "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  #NOT USED\n",
        "        file_range = range(*file_range) if file_range is not None else None \n",
        "        file_names = glob.glob(file_patterns)\n",
        "        mesra_collection = []\n",
        "        for file_name in file_names:\n",
        "            if normalize:\n",
        "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
        "            else:\n",
        "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
        "        return mesra_collection\n",
        "    \n",
        "\n",
        "    def read_documents(self): #NOT USED\n",
        "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
        "        cleaned = self.clean_data(poems)\n",
        "        return ' \\n '.join(cleaned)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5f754d9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MesraGenerator:\n",
        "    def __init__(self, train_path, test_path):\n",
        "        self.base_path = \"./data\"\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.generator = None\n",
        "        self.train_path = os.path.join(self.base_path, train_path)\n",
        "        self.test_path = os.path.join(self.base_path, test_path)\n",
        "        # self.cleaner = Cleaner()\n",
        "\n",
        "    def read_data(self, tokenizer, train_path=None, test_path=None):\n",
        "        path = train_path if train_path is not None else self.train_path\n",
        "        test_path = test_path if test_path is not None else self.test_path\n",
        "        train_dataset = TextDataset(\n",
        "            tokenizer=tokenizer, file_path=path, block_size=128)\n",
        "        \n",
        "        test_dataset = TextDataset(\n",
        "            tokenizer=tokenizer, file_path=test_path, block_size=128)\n",
        "        \n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "        # train_texts, val_texts = train_test_split(train_dataset)\n",
        "        return train_dataset, test_dataset, data_collator\n",
        "\n",
        "    def read_model(self, model_type='bolbolzaban/gpt2-persian'):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "        return self.model, self.tokenizer\n",
        "\n",
        "    def freeze_lower_layers(self):\n",
        "        for param in self.model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in (\n",
        "            self.model.base_model.h[23].parameters() or self.model.base_model.h[22].parameters()\n",
        "        ):\n",
        "            param.requires_grad = True\n",
        "            \n",
        "    def fine_tune_model(self, model, tokenizer, train_texts, val_texts, data_collator):\n",
        "        training_args = TrainingArguments(\n",
        "        output_dir=\"./model\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "        # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "        per_device_train_batch_size=12,\n",
        "        per_device_eval_batch_size=12,\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        warmup_steps=500)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_texts,\n",
        "            eval_dataset=val_texts,\n",
        "        )\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "        \n",
        "    def init_generator(self):      \n",
        "        model, tokenizer = self.read_model()\n",
        "        self.freeze_lower_layers()\n",
        "        train_texts, val_texts, data_collator = self.read_data(tokenizer)\n",
        "        trainer = self.fine_tune_model(model, tokenizer, train_texts, val_texts, data_collator)\n",
        "        generator = pipeline('text-generation', trainer.model, tokenizer=tokenizer, config={'max_length':256}, device=0)\n",
        "        self.generator = generator\n",
        "        return generator\n",
        "\n",
        "    def save_model(self, dir=None):\n",
        "        self.trainer.save_model(output_dir=dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9b0bd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b0bd15",
        "outputId": "0116c322-0947-4283-8522-2682d531889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49606\n"
          ]
        }
      ],
      "source": [
        "cleaner = Cleaner()\n",
        "cleaner.add_statement_mesras()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e56563",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = \"f_beyt.txt\"\n",
        "test_path = \"test.txt\"\n",
        "mesra_model = MesraGenerator(train_path, test_path)\n",
        "generator = mesra_model.init_generator()\n",
        "# train_texts, val_texts, data_collator = generator.read_data()\n",
        "generator('[BOM]به گرد اندر آرد بهنگام کار')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
