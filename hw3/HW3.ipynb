{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2857862f",
      "metadata": {},
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        ØªÙ…Ø±ÛŒÙ† Ø³ÙˆÙ… Ø¯Ø±Ø³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ\n",
        "    </h1>\n",
        "    <h3>\n",
        "        Ú¯Ø±Ø¯Ø¢ÙˆØ±Ù†Ø¯Ú¯Ø§Ù†:<br/>\n",
        "        Ø³Ø§Ø­Ù„ Ù…Ø³â€ŒÙØ±ÙˆØ´ØŒ Ø³Ø±ÙˆØ´ ØªØ§Ø¨Ø´ØŒ Ø¯Ø±Ù†Ø§ Ø¯Ù‡Ù‚Ø§Ù†ÛŒ\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† ØªØ±Ú© \"ØªÚ©Ù…ÛŒÙ„ Ù…ØµØ±Ø§Ø¹ Ø¯ÙˆÙ… Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª ÙˆØ²Ù† Ø´Ø¹Ø±\" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ú©Ù…Ú© Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ù…Ù†Ø§Ø³Ø¨ØŒ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…ØµØ±Ø¹ Ø§Ø² Ø¨ÛŒØª Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ú¯ÛŒØ±ÛŒÙ… Ùˆ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ØµØ±Ø¹ Ø¯ÙˆÙ… Ø§ÛŒÙ† Ø¨ÛŒØª Ø±Ø§ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ù‡ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ù…Ø§ Ø§Ø² ÙˆØ¨Ø³Ø§ÛŒØª Ú¯Ù†Ø¬ÙˆØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ú©Ù…Ú© Ú¯Ø±ÙØªÛŒÙ… Ùˆ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† Ù‚Ø§Ù„Ø¨ Ø´Ø¹Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø²ÛŒØ±Ø§ Ù‚Ø§ÙÛŒÙ‡ Ø¯Ø§Ø´ØªÙ† ÛŒØ§ Ù†Ø¯Ø§Ø´ØªÙ† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù…â€ŒÙ‚Ø§ÙÛŒÙ‡ Ø¨ÙˆØ¯Ù† Ø¯Ùˆ Ù…ØµØ±Ø§Ø¹ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù‚Ø§Ù„Ø¨ Ù…Ø«Ù†ÙˆÛŒ Ù†Ø¸ÛŒØ± Ø´Ø§Ù‡Ù†Ù…Ø§Ù‡â€ŒÛŒ ÙØ±Ø¯ÙˆØ³ÛŒ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…Ø› Ùˆ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù…â€ŒÙ‚Ø§ÙÛŒÙ‡ Ø¨ÙˆØ¯Ù† Ø¯Ùˆ Ù…ØµØ±Ø§Ø¹ Ù†Ø¨ÙˆØ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ù‚Ø§Ù„Ø¨ Ù‚Ø·Ø¹Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. <br>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ú†Ù†ÛŒÙ† Ø³ÛŒØ³ØªÙ…ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…: Ù…Ø¯Ù„ n-gramØŒ Ù…Ø¯Ù„ encoder-decoder Ø¨Ø§ ÛŒÚ© Ø´Ø¨Ú©Ù‡ LSTM Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† encoderØŒ Ùˆ Ù…Ø¯Ù„ÛŒ Ù¾ÛŒØ±Ùˆ Ù…Ú©Ø§Ù†ÛŒØ³Ù… ØªÙˆØ¬Ù‡.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0571e",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. Ø¯ÙˆØ¨Ø§Ø±Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø¯Ø§Ø¯Ù‡ </h3>\n",
        "    Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø§Ø¨ÛŒØ§Øª Ùˆ Ù…ØµØ±Ø§Ø¹â€ŒÙ‡Ø§ Ù…Ø´Ø®Øµ Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ú©Ù…Ú© Ú©Ø¯ Ø²ÛŒØ± Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ø±Ø¯ÛŒÙ…. Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø§Ø¨ÛŒØ§Øª Ø¨Ø§ __BOB__ Ùˆ __EOB__ Ùˆ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØµØ±Ø§Ø¹â€ŒÙ‡Ø§ Ø¨Ø§ __BOM__ Ùˆ __EOM__ Ù…Ø´Ø®Øµ Ú¯Ø±Ø¯ÛŒØ¯.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ab86f6b",
      "metadata": {
        "id": "9ab86f6b"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3251a5c1",
      "metadata": {
        "id": "3251a5c1"
      },
      "outputs": [],
      "source": [
        "path_dir = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed4fe8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4fe8e2",
        "outputId": "de1a16d9-f71f-4d5e-a483-65c9b2c5bc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: hazm in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Requirement already satisfied: transformers in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: requests in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (4.63.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: torch in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8425cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8425cfb",
        "outputId": "1d76e685-8a26-4380-947e-7d7f6315b89c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1902c48a",
      "metadata": {
        "id": "1902c48a"
      },
      "outputs": [],
      "source": [
        "class Cleaner:\n",
        "    def __init__(self):\n",
        "        self.normalizer = Normalizer()\n",
        "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
        "        self.file_range = None\n",
        "\n",
        "    def add_statement_mesras(self, file_out='f_beyt.txt'):\n",
        "        start_mesra = '[BOM]'\n",
        "        end_mesra = ''\n",
        "        start_beyt = ''\n",
        "        end_beyt = '[EOS]' + os.linesep\n",
        "        beyt_file = ''\n",
        "        with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "            lines = fp.readlines()\n",
        "            for i in range(0, len(lines) - 1, 2):\n",
        "                mesra1 = start_mesra + lines[i].strip() + end_mesra\n",
        "                mesra2 = start_mesra + lines[i + 1].strip() + end_mesra\n",
        "                b = start_beyt + mesra1.strip() + ' ' + mesra2.strip() + end_beyt\n",
        "                beyt_file += b\n",
        "                \n",
        "        with open(file_out, 'w', encoding=\"utf-8\") as fp:\n",
        "            fp.write(beyt_file)\n",
        "\n",
        "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  \n",
        "        file_range = range(*file_range) if file_range is not None else None \n",
        "        file_names = glob.glob(file_patterns)\n",
        "        mesra_collection = []\n",
        "        for file_name in file_names:\n",
        "            if normalize:\n",
        "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
        "            else:\n",
        "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
        "        return mesra_collection\n",
        "    \n",
        "\n",
        "    def read_documents(self):\n",
        "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
        "        cleaned = self.clean_data(poems)\n",
        "        return ' \\n '.join(cleaned)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f754d9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel\n",
        ")\n",
        "\n",
        "class MesraGenerator:\n",
        "    def __init__(self, path):\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.train_path = path\n",
        "        self.cleaner = Cleaner()\n",
        "\n",
        "    def read_data(self):\n",
        "        train_dataset = TextDataset(\n",
        "            tokenizer=tokenizer, file_path=self.train_path, block_size=256)\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "        return train_dataset, data_collator\n",
        "\n",
        "    def read_model(self, model_type='bolbolzaban/gpt2-persian'):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "\n",
        "    def freeze_lower_layers(self):\n",
        "        for param in self.model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in (\n",
        "            self.model.base_model.h[23].parameters() or self.model.base_model.h[22].parameters()\n",
        "        ):\n",
        "            param.requires_grad = True\n",
        "            \n",
        "    def fine_tune_model(self):\n",
        "        training_args = TrainingArguments(\n",
        "        output_dir=\"./model\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "        # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "        per_device_train_batch_size=12,\n",
        "        per_device_eval_batch_size=12,\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        warmup_steps=500)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_texts,\n",
        "            eval_dataset=val_texts,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "    def save_model(self, dir=None):\n",
        "        self.trainer.save_model(output_dir=dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e9b0bd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b0bd15",
        "outputId": "0116c322-0947-4283-8522-2682d531889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99217\n"
          ]
        }
      ],
      "source": [
        "path=f\"{path_dir}/ferdousi.txt\"\n",
        "cleaner = Cleaner()\n",
        "cleaner.add_statement_mesras()\n",
        "mesras = cleaner.read_poems(file_patterns=path, file_range=None, normalize=False)\n",
        "print(len(mesras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5e8521",
      "metadata": {
        "id": "dd5e8521",
        "outputId": "3b0a2062-3405-4cb2-e56f-f513e1d75b9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 452M/452M [03:31<00:00, 2.23MB/s]   \n",
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Ù…Ø§',\n",
              " 'Ø¯Ø±',\n",
              " 'Ù‡ÙˆØ´',\n",
              " '[ZWNJ]',\n",
              " 'ÙˆØ§Ø±Ù‡',\n",
              " 'Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ…',\n",
              " 'Ø¨Ø§',\n",
              " 'Ø§Ù†ØªÙ‚Ø§Ù„',\n",
              " 'ØµØ­ÛŒØ­',\n",
              " 'Ø¯Ø§Ù†Ø´',\n",
              " 'Ùˆ',\n",
              " 'Ø¢',\n",
              " '##Ú¯Ø§Ù‡ÛŒ',\n",
              " 'ØŒ',\n",
              " 'Ù‡Ù…Ù‡',\n",
              " 'Ø§ÙØ±Ø§Ø¯',\n",
              " 'Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯',\n",
              " 'Ø§Ø²',\n",
              " 'Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ',\n",
              " 'Ù‡ÙˆØ´Ù…Ù†Ø¯',\n",
              " 'Ø§Ø³ØªÙØ§Ø¯Ù‡',\n",
              " 'Ú©Ù†Ù†Ø¯',\n",
              " '.',\n",
              " 'Ø´Ø¹Ø§Ø±',\n",
              " 'Ù…Ø§',\n",
              " 'Ù‡ÙˆØ´',\n",
              " 'Ù…ØµÙ†ÙˆØ¹ÛŒ',\n",
              " 'Ø¨Ø±Ø§ÛŒ',\n",
              " 'Ù‡Ù…Ù‡',\n",
              " 'Ø§Ø³Øª',\n",
              " '.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
        "\n",
        "# v3.0\n",
        "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
        "model = AutoModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "text = \"Ù…Ø§ Ø¯Ø± Ù‡ÙˆØ´â€ŒÙˆØ§Ø±Ù‡ Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ… Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ ØµØ­ÛŒØ­ Ø¯Ø§Ù†Ø´ Ùˆ Ø¢Ú¯Ø§Ù‡ÛŒØŒ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯. Ø´Ø¹Ø§Ø± Ù…Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø³Øª.\"\n",
        "tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7dd04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7dd04d",
        "outputId": "da16e9d2-bad0-4973-8ac7-f7117de1fbb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Freezing the lower layers increases the training speed and reduces the memory requirement.\n",
        "# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n",
        "# or unfreeze as many layers that you can affort training with a reasonable batchsize.\n",
        "\n",
        "\n",
        "\n",
        "# load model\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "\n",
        "# freeze lower layers and only train top layers\n",
        "freeze_lower_layers()\n",
        "\n",
        "# load dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "train_dataset, test_dataset, data_collator = load_dataset(\n",
        "    \"ferdousi.txt\", \"test.txt\", tokenizer\n",
        ")\n",
        "print(type(train_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "gXpjLVYYaXjQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpjLVYYaXjQ",
        "outputId": "bdc3fedc-9e85-40c5-af47-5ebc13776b90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'freeze_lower_layers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbolbolzaban/gpt2-persian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbolbolzaban/gpt2-persian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=3'>4</a>\u001b[0m freeze_lower_layers()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=4'>5</a>\u001b[0m generator \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m'\u001b[39m, model, tokenizer\u001b[39m=\u001b[39mtokenizer, config\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m256\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000013?line=5'>6</a>\u001b[0m sample \u001b[39m=\u001b[39m generator(\u001b[39m'\u001b[39m\u001b[39mØ¯Ø± ÛŒÚ© Ø§ØªÙØ§Ù‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ²ØŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù†\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'freeze_lower_layers' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "\n",
        "freeze_lower_layers()\n",
        "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
        "sample = generator('Ø¯Ø± ÛŒÚ© Ø§ØªÙØ§Ù‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ²ØŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù†')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OyvnTBDCdH3s",
      "metadata": {
        "id": "OyvnTBDCdH3s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aLCtmbDhbkPB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCtmbDhbkPB",
        "outputId": "c1948abf-ad6d-4c41-9509-b7f7590e7016"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000015?line=0'>1</a>\u001b[0m generator(\u001b[39m'\u001b[39m\u001b[39m ØªÙˆØ§Ù†Ø§ Ø¨ÙˆØ¯\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
          ]
        }
      ],
      "source": [
        "generator(' ØªÙˆØ§Ù†Ø§ Ø¨ÙˆØ¯')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9sdNgE4abLmi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sdNgE4abLmi",
        "outputId": "c9e604cb-44e7-414e-bed5-56a732f1635c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Ø¯Ø± ÛŒÚ© Ø§ØªÙØ§Ù‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ²ØŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ ØŒ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø´Ù Ø¢Ù† Ø´Ø¯Ù†Ø¯.'}]\n"
          ]
        }
      ],
      "source": [
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d906e9",
      "metadata": {
        "id": "64d906e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts = train_test_split(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156a8c43",
      "metadata": {
        "id": "156a8c43",
        "outputId": "17e5440f-9532-4edc-fb1f-3c69bb594aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03004f61",
      "metadata": {
        "id": "03004f61",
        "outputId": "5809efaf-8475-4ba9-f79e-63aae641d416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(val_texts))\n",
        "print(type(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936486e8",
      "metadata": {
        "id": "936486e8",
        "outputId": "149b9b9c-b3ef-449b-9d04-bc0112636736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1895\n",
            "632\n",
            "2527\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9ff25dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "9ff25dd9",
        "outputId": "28249912-10a7-41a0-9f22-c3b43be8e8b5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TrainingArguments' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/sahel/university/NLP/hws/hw3/HW3.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=1'>2</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=3'>4</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=4'>5</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=5'>6</a>\u001b[0m     \u001b[39m# Set the batch size to a maximum value that could fit into GPU memory,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=6'>7</a>\u001b[0m     \u001b[39m# for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=7'>8</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=8'>9</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=9'>10</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=10'>11</a>\u001b[0m     save_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=11'>12</a>\u001b[0m     warmup_steps\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=14'>15</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=16'>17</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=19'>20</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_texts,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sahel/university/NLP/hws/hw3/HW3.ipynb#ch0000021?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
          ]
        }
      ],
      "source": [
        "# train\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_texts,\n",
        "    eval_dataset=val_texts,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NV73xepfgy90",
      "metadata": {
        "id": "NV73xepfgy90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
