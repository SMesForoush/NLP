{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4ZKRc0M72-"
      },
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        تمرین سوم درس پردازش زبان‌های طبیعی\n",
        "    </h1>\n",
        "    <h3>\n",
        "        گردآورندگان:<br/>\n",
        "        ساحل مس‌فروش، سروش تابش، درنا دهقانی\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    ما در این تمرین ترک \"تکمیل مصراع دوم با رعایت وزن شعر\" را انتخاب کردیم. در این تمرین قصد داریم به کمک مدل‌های زبانی مناسب، چند کلمه به عنوان یک مصرع از بیت را به عنوان ورودی بگیریم و چند کلمه به عنوان مصرع دوم این بیت را خروجی دهیم. در این تمرین ما از وبسایت گنجور برای جمع‌آوری داده کمک گرفتیم و از چندین قالب شعری استفاده کردیم، زیرا قافیه داشتن یا نداشتن به عنوان ورودی داده می‌شود. اگر نیاز به هم‌قافیه بودن دو مصراع بود، از قالب مثنوی نظیر شاهنامه‌ی فردوسی کمک می‌گیریم و اگر نیاز به هم‌قافیه بودن دو مصراع نبود می‌توانیم از قالب غزل نظیر اشعار حافظ استفاده کنیم. <br>\n",
        "    برای ساخت چنین سیستمی از چند مدل زبانی استفاده می‌کنیم: مدل n-gram، مدل encoder-decoder با یک شبکه LSTM به عنوان encoder، و مدلی پیرو مکانیسم توجه.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbRSi4pJM73p",
        "outputId": "a62ecd89-1d2d-49f0-8623-62b9da331798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 13.2 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=83f9fb7c7c82fd27f58b62f76e39986a2c49d02bf4f7016db347014f05caac7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153858 sha256=84f28949d5f95f2385c48f838258f35cd0ab77a29dba92319cb7f11b39e7a51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 14.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement codec (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for codec\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 13.3 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=5050ffc15d2c1125d5e44a1d800c7defad35e48cd73323b8f6e6a0e02fe380c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 12.4 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.5\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 86.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 53.6 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.13\n",
            "    Uninstalling multiprocess-0.70.13:\n",
            "      Successfully uninstalled multiprocess-0.70.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.12.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch\n",
        "! pip install nltk\n",
        "! pip install numpy\n",
        "! pip install codec\n",
        "! pip install wandb\n",
        "! pip install datasets\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BHsgv30-M73w"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import time\n",
        "import copy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmvI4S4AM73y"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. دوباره‌نویسی داده </h3>\n",
        "    از آنجایی که در داده‌ی گرفته شده، ابتدا و انتهای ابیات و مصراع‌ها مشخص نبود، به کمک کد زیر این داده را بازنویسی کردیم. ابتدای هر مصرع جدید با __BOM__ و پایان بیت با __EOM__ مشخص گردید.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJFgOnmUM730"
      },
      "outputs": [],
      "source": [
        "path = 'all_norm.txt'\n",
        "beyt_file = ''\n",
        "with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "    lines = fp.readlines()\n",
        "    for i in range(0, len(lines) - 1, 2):\n",
        "        mesra1 = '__BOM__ ' + lines[i].strip()\n",
        "        mesra2 = '__BOM__ ' + lines[i + 1].strip()\n",
        "        b = mesra1.strip() + ' ' + mesra2.strip() + ' __EOM__\\n'\n",
        "        beyt_file += b\n",
        "        \n",
        "with open('all_beyt.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "    fp.write(beyt_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDA1FO14M732"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 2. مدل n-gram </h3>\n",
        "    برای ساخت این مدل از کلاس Ngram استفاده کردیم که به کمک مطالب گفته شده در کلاس نوشته شده است. <br> در این مدل، دیتای لازم برای train، مقدار مورد استفاده برای laplace smoothing و n به عنوان ورودی گرفته می‌شوند و بر اساس آنها مدل ساخته می‌شود. هنگام train کردن، اگر با token مواجه شدیم که تکرار آن در کل دیتا بیش از 1 نبود از عبارت __UNK__ به جای آن استفاده می‌کنیم. سپس احتمال هر n-gram متناسب با تعداد آن در کل و تعداد (n_1)-gram ها محاسبه شده و laplace smoothing روی آن انجام می‌گیرد. نهایتاً n-gramها با توجه به احتمالشان به طور کاهشی مرتب می‌شوند، تا سرعت یافتن مصراع دوم بالاتر رود.<br> سپس یک مصراع به عنوان ورودی گرفته می‌شود. با توجه به n، n_1 token پایانی مصراع ورودی پیش پردازش می‌شود و بین n-gramهای موجود که شروعشان با آن n_1 token باشد، n-gram با بیشترین احتمال انتخاب شده و token پایانی آن به عنوان کلمه خروجی داده می‌شود. به همین ترتیب token تولید می‌شود تا به __EOM__ برخورد کنیم.<br>این مدل unigram را نیز اجرا می‌کند و بدین صورت عمل می‌کند که به تعداد کلمات معمول یک مصراع شاهنامه، یعنی 6، متناسب با احتمال هر unigram آنها را تولید می‌کند. واضحاً چون با این روش هر کلمه مستقل از دیگری تولید می‌شود، نتیجه قابل قبول نیست.<br>یک boolean به عنوان ورودی تابع generate_mesra برای رعایت یا عدم رعایت قافیه در نظر گرفته می‌شود. متناسب با مقدار این boolean پس از ساخت مصراع دوم، کلمه‌ی آخر آن بررسی می‌گردد؛ به طور مثال اگر این boolean مقدار True داشت و دو مصراع هم‌قافیه نبودند، کلمه‌ی آخر مصراع ساخته شده به تعداد بار محدودی مجدداً ساخته می‌شود پس از هر بار ساخت مجدداً هم‌قافیه بودن بررسی می‌گردد. در صورتی‌که پس از این تعداد بار محدود هم‌چنان دو مصراع به شرط قافیه‌ی گفته شده نرسیده بودند، دیگر به این شرط توجهی نمی‌شود و شعر ساخته شده‌ی نهایی فارغ از هم‌قافیه بودن یا نبودن خروجی داده می‌شود.<br>بررسی هم‌قافیه بودن با مقایسه‌ی 2 حرف آخر دو کلمه انجام می‌گیرد. اگر دو حرف آخر دو کلمه یکسان بودند هم‌قافیه‌اند.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk6DhWdFM735"
      },
      "outputs": [],
      "source": [
        "class Ngram(object):\n",
        "    \n",
        "    UNK = '__UNK__'\n",
        "    BOM = '__BOM__'\n",
        "    EOM = '__EOM__'\n",
        "    \n",
        "    def __init__(self, data, n, laplace=1):\n",
        "        self.vocab = dict()\n",
        "        self.n = n\n",
        "        self.laplace = laplace\n",
        "        self.tokens = self.preprocess(data)\n",
        "        self.vocab = nltk.FreqDist(self.tokens)\n",
        "        self.model = self.create_model()\n",
        "    \n",
        "    def preprocess(self, data):\n",
        "        tokens = data.strip().split()\n",
        "        if not len(self.vocab):\n",
        "            self.vocab = nltk.FreqDist(tokens)\n",
        "        return [token if self.vocab[token] > 1 else Ngram.UNK for token in tokens]\n",
        "    \n",
        "    def create_model(self):\n",
        "        num_tokens = len(self.tokens)\n",
        "        if self.n == 1:\n",
        "            model = { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
        "        else:\n",
        "            model = self.smooth()\n",
        "        return dict(sorted(model.items(), key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "    def smooth(self):\n",
        "        vocab_len = len(self.vocab)\n",
        "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
        "        n_vocab = nltk.FreqDist(n_grams)\n",
        "        m_grams = nltk.ngrams(self.tokens, self.n - 1)\n",
        "        m_vocab = nltk.FreqDist(m_grams)\n",
        "        \n",
        "        def counted_smoothing(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[m_gram]\n",
        "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_len)\n",
        "        \n",
        "        return { n_gram: counted_smoothing(n_gram, count) for n_gram, count in n_vocab.items() }\n",
        "    \n",
        "    def check_ghafie(self, ghafie1, ghafie2):\n",
        "        margin = 3\n",
        "        ham_ghafie = False\n",
        "        if len(ghafie1) > 1 and len(ghafie2) > 1:\n",
        "            for i in range(-1, -margin-1, -1):\n",
        "                if ghafie1[i] != ghafie2[i]:\n",
        "                    break\n",
        "                else:\n",
        "                    ham_ghafie = True\n",
        "        return ham_ghafie\n",
        "        \n",
        "    \n",
        "    def get_candidates(self, beginning_of_ngram, last_word, ghafie, ghafie_repeat, mesra1_ghafie):\n",
        "        counter = 0\n",
        "        best = None\n",
        "        for ng in self.model.items():\n",
        "            for i in range(self.n - 1):\n",
        "                if ng[0][i] != beginning_of_ngram[i]:\n",
        "                    break\n",
        "            else:\n",
        "                if ng[0][-1] != self.BOM and ng[0][-1] != self.UNK:\n",
        "                    if not last_word:\n",
        "                        return ng[0][-1]\n",
        "                    if counter == 0:\n",
        "                        best = ng[0][-1]\n",
        "                    if ((ghafie and not self.check_ghafie(mesra1_ghafie, ng[0][-1])) or (not ghafie and self.check_ghafie(mesra1_ghafie, ng[0][-1]))) and counter < ghafie_repeat:\n",
        "                        counter += 1\n",
        "                    else:\n",
        "                        if counter < ghafie_repeat:\n",
        "                            return ng[0][-1]\n",
        "        if best:\n",
        "            return best\n",
        "        return self.UNK\n",
        "    \n",
        "    def generate_mesra(self, first_mesra, ghafie):\n",
        "        first_mesra = self.BOM + ' ' + first_mesra + ' ' + self.BOM\n",
        "        first_mesra_tokens = self.preprocess(first_mesra)\n",
        "        next_mesra = []\n",
        "        ghafie_repeat = 5\n",
        "        mesra1_ghafie = first_mesra_tokens[-2]\n",
        "                \n",
        "        if self.n == 1:\n",
        "            vocab_list = [x[0] for x in self.model]\n",
        "            vocab_probs = list(self.model.values())\n",
        "            next_mesra = list(np.random.choice(vocab_list, p=vocab_probs, size=8, replace=False))\n",
        "            \n",
        "            if ghafie:\n",
        "                if not self.check_ghafie(first_mesra_tokens[-2], next_mesra[-1]):\n",
        "                    for i in range(ghafie_repeat):\n",
        "                        next_mesra[-1] = np.random.choice(vocab_list, p=vocab_probs)\n",
        "                        if self.check_ghafie(first_mesra_tokens[-2], next_mesra[-1]):\n",
        "                            break\n",
        "            else:\n",
        "                if self.check_ghafie(first_mesra_tokens[-2], next_mesra[-1]):\n",
        "                    for i in range(ghafie_repeat):\n",
        "                        next_mesra[-1] = np.random.choice(vocab_list, p=vocab_probs)\n",
        "                        if not self.check_ghafie(first_mesra_tokens[-2], next_mesra[-1]):\n",
        "                            break\n",
        "                \n",
        "        \n",
        "        else:\n",
        "            beginning_of_ngram = tuple(first_mesra_tokens[-self.n + 1:])\n",
        "            last_word = False\n",
        "            while True:\n",
        "                next_word = self.get_candidates(beginning_of_ngram, last_word, ghafie, ghafie_repeat, mesra1_ghafie)\n",
        "                \n",
        "                if next_word == self.EOM:\n",
        "                    last_word = True\n",
        "                    next_mesra[-1] = self.get_candidates(temp_beginning_of_ngram, last_word, ghafie, ghafie_repeat, mesra1_ghafie)\n",
        "                    break\n",
        "        \n",
        "                next_mesra.append(next_word)\n",
        "                temp_beginning_of_ngram = beginning_of_ngram\n",
        "                beginning_of_ngram = (*beginning_of_ngram[1:], next_word)\n",
        "                \n",
        "        while self.EOM in next_mesra:\n",
        "            next_mesra.remove(self.EOM)\n",
        "        while self.BOM in next_mesra:\n",
        "            next_mesra.remove(self.BOM)\n",
        "        while self.UNK in next_mesra:\n",
        "            next_mesra.remove(self.UNK)\n",
        "        return(' '.join(next_mesra))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG3s_L-TM73-"
      },
      "outputs": [],
      "source": [
        "gram5 = Ngram(beyt_file, 5)\n",
        "gram4 = Ngram(beyt_file, 4)\n",
        "gram3 = Ngram(beyt_file, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LsN3lAnM74A",
        "outputId": "cf2c32d0-acad-45c7-f780-31d61c272a8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ز دینار وز گوهر نابسود'"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'به گرد اندر آرد بهنگام کار'\n",
        "gram4.generate_mesra(mesra1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SVOH37pM74C",
        "outputId": "04b20054-e9d5-45c3-e22d-e475c89dd86f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ز دینار وز گوهر شاهوار'"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gram4.generate_mesra(mesra1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_0ZCckAM74E",
        "outputId": "b19a8582-b46c-479b-b06f-170b97fc6400"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'سپردار و جوشنوران صد هزار'"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'سپاه اندر آمد به پیش سوار'\n",
        "gram4.generate_mesra(mesra1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQGN56EHM74H",
        "outputId": "5febcaca-eaf6-461e-8fb7-f01ac58a3abb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'سپردار و جوشنوران صد هزار'"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gram4.generate_mesra(mesra1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHyNeOO8M74K",
        "outputId": "41fcbc98-3f2d-423d-ee3d-253ba1caa1ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'همی رفت پویان به کردار مست'"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'بران تیغ زهر آب داده به دست'\n",
        "gram5.generate_mesra(mesra1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-a023N7M74N",
        "outputId": "e6370c21-d015-47d2-96fb-32d1e58aba66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'گفت ببخشند گنه می بنوش'"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'هاتفی از گوشه میخانه دوش'\n",
        "gram5.generate_mesra(mesra1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9n_MD-6M74O",
        "outputId": "a2e2eafc-330d-4eea-8c48-48254a09cb53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ز هر سو که بد مهتری با گهر'"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'هزار دشمنم ار می کنند قصد هلاک'\n",
        "gram3.generate_mesra(mesra1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xBzeup-M74Q",
        "outputId": "21b015b7-a978-499d-ac19-b158e3d4b135"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'بدان تا شود نزد سالار توران سپاه'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesra1 = 'سلامی چو بوی خوش آشنایی'\n",
        "gram3.generate_mesra(mesra1, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkRZvszYM74U"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 3. مدل بر پایه LSTM </h3>\n",
        "    ابتدا بررسی می‌کنیم که سیستم قابلیت استفاده از cuda را دارد یا نه.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar1z2v-kM74V",
        "outputId": "b5c8a5fe-7db7-49ec-cd01-308d325107c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Persian_poems_corpus'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (159/159), 45.21 MiB | 19.40 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Checking out files: 100% (148/148), done.\n",
            "cp: target 'Persian_poems_corpus/normalized/moulavi_norm.txt' is not a directory\n",
            "cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "! pip install -q wandb\n",
        "! git clone \"https://github.com/amnghd/Persian_poems_corpus.git\"\n",
        "! mkdir \"corpus\"\n",
        "! cp \"Persian_poems_corpus/normalized/ferdousi_norm.txt\" \"Persian_poems_corpus/normalized/hafez_norm.txt\" \"Persian_poems_corpus/normalized/moulavi_norm.txt\"\n",
        "\"./corpus/\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# for accessing the files in google drive from google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iu42AaBM74Y"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    برای مانیتور کردن پارامترها و تغییرات مدل هنگام train شدن می‌توانیم از wandb استفاده کنیم. هم‌چنین امکان ذخیره‌ی پارامترها برای استفاده‌ی مجدد نیز وجود دارد. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Qnr-r-lSM74a",
        "outputId": "6adc1742-7d1d-4ac2-e96e-81dce509b796"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: thedrna (nlp400). Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>F:\\Documents\\NLP\\HW 3\\wandb\\run-20220605_134248-1tw9umfa</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nlp400/ferdousi-generator/runs/1tw9umfa\" target=\"_blank\">valiant-waterfall-2</a></strong> to <a href=\"https://wandb.ai/nlp400/ferdousi-generator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nlp400/ferdousi-generator/runs/1tw9umfa?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x1c07627ed90>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"ferdousi-generator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcmGR6KoM74d"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    pass\n",
        "\n",
        "\n",
        "wandb_active = False\n",
        "project_name = 'poem_generator'\n",
        "run_name = 'all_poem_train'\n",
        "checkpoints_dir = '../data/checkpoints/'\n",
        "corpus_dir = '../data/poems/'\n",
        "vocab_path = '../data/vocabulary.txt'\n",
        "\n",
        "if wandb_active:\n",
        "    wandb.init(project=project_name, name=run_name)\n",
        "    config = wandb.config\n",
        "else:\n",
        "    config = Config()\n",
        "config.batch_size = 256\n",
        "config.embedding_size = 512\n",
        "config.lstm_num_layers = 3\n",
        "config.lstm_hidden_size = 512\n",
        "config.sequence_length = 10\n",
        "config.log_interval = 10\n",
        "config.learning_rate = 0.001\n",
        "config.vocab_size = 38590\n",
        "config.lstm_dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JU3e_YbM74e"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    در کلاس PoemDataset به خواندن و ایندکس کردن دیتا می‌پردازیم. هم‌چنین کلمات یکتای دیتا برای تشکیل vocabulary استخراج می‌گردند. برای هر کلمه‌ی یکتا، ایندکسی در نظر گرفته می‌شود و توسط تابع __getitem__ داده‌ای مناسب ورودی دادن به مدل گرفته می‌شود. از index_to_word و word_to_index در انکد و دیکد کردن استفاده می‌شود.<br>به کمک این کلاس می‌توان از میان داده‌های موجود، ابتدا به کمک تمامی اشعار  مجموعه‌ی واژگان کاملی ساخت و مدل را به کمک آن train کرد، سپس بر حسب هر شاعر آن را fine tune کرد. هم‌چنین برای پیش‌پردازش اشعار، ابتدا و انتهای مصراع‌ها و ابیات به همراه نام شاعر مشخص می‌گردد.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v7N6AxpM74f"
      },
      "outputs": [],
      "source": [
        "class PoemDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            config,\n",
        "            device=torch.device('cpu'),\n",
        "            poet='ferdousi',\n",
        "            corpus_dir='./Persian_poems_corpus/normalized',\n",
        "            vocab_path='./vocabulary.txt'\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.corpus_dir = corpus_dir\n",
        "        self.vocab_path = vocab_path\n",
        "\n",
        "        self.words_by_poet = self.load_words(corpus_dir)\n",
        "        self.vocabulary = self.load_vocabulary()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.vocabulary)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.vocabulary)}\n",
        "        self.poet = poet\n",
        "\n",
        "    def preprocess_lines(self, lines, mask_key):\n",
        "        lines = [line.strip() for line in lines]\n",
        "        lines = filter(lambda line: len(line) > 0, lines)\n",
        "        lines = map(lambda line: line.replace('\\n', ''), lines)\n",
        "        lines = map(lambda line: line.replace('\\t', ''), lines)\n",
        "        lines = map(lambda line: line.replace('\\r', ''), lines)\n",
        "        lines = map(\n",
        "            lambda index_line:\n",
        "            f'[BOM_{mask_key}] ' + index_line[1] + ' [EOS]' if index_line[0] % 2 == 1\n",
        "            else f'[BOM_{mask_key}] ' + index_line[1],\n",
        "            enumerate(lines)\n",
        "        )\n",
        "        words = itertools.chain.from_iterable(map(lambda line: line.split(' '), lines))\n",
        "        words = filter(lambda word: len(word) > 0, words)\n",
        "        words = list(words)\n",
        "        return words\n",
        "\n",
        "    def load_words(self, corpus_dir):\n",
        "        words_by_poet = {}\n",
        "        for filename in os.listdir(corpus_dir):\n",
        "            with open(os.path.join(corpus_dir, filename)) as f:\n",
        "                poet_name = filename.split('_')[0]\n",
        "                lines = f.readlines()\n",
        "                words_by_poet[poet_name] = self.preprocess_lines(lines, poet_name)\n",
        "        return words_by_poet\n",
        "\n",
        "    def load_vocabulary(self):\n",
        "        with open(self.vocab_path) as f:\n",
        "            vocabulary = f.readlines()\n",
        "        vocabulary = [word.strip() for word in vocabulary]\n",
        "        return vocabulary\n",
        "\n",
        "    @property\n",
        "    def all_poets(self):\n",
        "        return self.words_by_poet.keys()\n",
        "\n",
        "    @property\n",
        "    def poet(self):\n",
        "        return self._poet\n",
        "\n",
        "    @poet.setter\n",
        "    def poet(self, poet):\n",
        "        self._poet = poet\n",
        "        if poet == 'all':\n",
        "            self.words = list(itertools.chain.from_iterable(self.words_by_poet.values()))\n",
        "        else:\n",
        "            self.words = self.words_by_poet[poet]\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.config.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tensors = (\n",
        "            torch.tensor(self.words_indexes[index:index + self.config.sequence_length]).to(self.device),\n",
        "            torch.tensor(self.words_indexes[index + 1:index + self.config.sequence_length + 1]).to(self.device),\n",
        "        )\n",
        "        return tensors\n",
        "\n",
        "\n",
        "dataset = PoemDataset(config, device=torch.device('cpu'), poet='all', corpus_dir=corpus_dir, vocab_path=vocab_path)\n",
        "\n",
        "for i in range(10):\n",
        "    print(dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A532FGpAM74i"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    در کلاس Model مدل این بخش ساخته می‌شود که شامل سه بخش embedding، lstm و یک تابع خطی برای تبدیل خروجی lstm به vocab است. لایه‌های هر بخش توسط pytorch ساخته می‌شوند و مقادیر ابتدایی برابر با صفر است.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITpvYYtFM74j"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config, device=torch.device('cpu')):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = config.embedding_size\n",
        "        self.lstm_hidden_size = config.lstm_hidden_size\n",
        "        self.lstm_dropout = 0.2\n",
        "        self.embedding_dim = config.embedding_size\n",
        "        self.num_layers = config.lstm_num_layers\n",
        "        self.device = device\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.vocab_size,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=self.lstm_dropout,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, self.vocab_size)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zcf2fxYM74l"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    نهایتاً یک تابع کلاسیک برای train داده‌ها استفاده می‌شود. در این تابع از optimizer جنریک Adam استفاده شده و loss به روش cross entropy محاسبه شده است. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to generate new vocabulary\n",
        "\n",
        "all_words = list(itertools.chain.from_iterable(dataset.words_by_poet.values()))\n",
        "word_counts = Counter(all_words)\n",
        "vocab = sorted(list(word_counts))\n",
        "with open(vocab_path, 'w') as f:\n",
        "    for word in vocab:\n",
        "        f.write(word + '\\n')"
      ],
      "metadata": {
        "id": "YqnfJbptjllX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uds74KajM74m"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, config, checkpoint_path='../data/checkpoints', max_epochs=10, ):\n",
        "    if wandb_active:\n",
        "        wandb.watch(model)\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=config.batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    print({'batch_count': len(dataloader), 'epoch_count': max_epochs})\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(config.sequence_length)\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print({'epoch': epoch, 'batch': batch, 'loss': loss.item()})\n",
        "            if wandb_active and batch % config.log_interval == 0:\n",
        "                wandb.log({\"loss\": loss})\n",
        "        try:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "            }, os.path.join(checkpoint_path, f'{run_name}_checkpoint_{epoch}_{time.time()}.pt'))\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3V9rRJiM74n"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    از تابع predict برای حدس ادامه‌ی بیت استفاده می‌گردد.<br> دیکودر مدل در این بخش قرار دارد، بطوریکه وزن‌های لازم برای انتخاب کلمه‌ی بعد از اعمال softmax روی hidden state آخرین مرحله حاصل می‌گردند. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQX_0yH9M74o"
      },
      "outputs": [],
      "source": [
        "def predict(dataset, model, text, max_predict_length=12):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    i = 0\n",
        "    while words[-1] != '[EOS]' and i < max_predict_length:\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "        i += 1\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "DkMC2iVkM74p",
        "outputId": "40900c60-9893-404d-9dbb-bf4a1e5ffd56"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ac7a527f62cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoemDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/poems'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# get first 10 items in dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c1293c44c2ae>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, device, poet, corpus_dir, vocab_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_by_poet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c1293c44c2ae>\u001b[0m in \u001b[0;36mload_words\u001b[0;34m(self, corpus_dir)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mwords_by_poet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mpoet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/poems'"
          ]
        }
      ],
      "source": [
        "model = Model(config, device)\n",
        "dataset = PoemDataset(config, device=device, poet='all', corpus_dir=corpus_dir, vocab_path=vocab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B6wwEsXhM74r"
      },
      "outputs": [],
      "source": [
        "# train on generic dataset\n",
        "train(dataset, model, config, checkpoint_path=checkpoints_dir)\n",
        "if wandb_active:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ferdousi = copy.deepcopy(model)\n",
        "dataset.poet = 'ferdousi'\n",
        "run_name = 'ferdousi_fine_tune'\n",
        "if wandb_active:\n",
        "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
        "train(dataset, model_ferdousi, config, checkpoint_path=checkpoints_dir)\n",
        "if wandb_active:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "vYrM9XYolA5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hafez = copy.deepcopy(model)\n",
        "dataset.poet = 'hafez'\n",
        "run_name = 'hafez_fine_tune'\n",
        "if wandb_active:\n",
        "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
        "train(dataset, model_hafez, config, checkpoint_path=checkpoints_dir)\n",
        "if wandb_active:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "IhnX6bCglCFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_moulavi = copy.deepcopy(model)\n",
        "dataset.poet = 'moulavi'\n",
        "run_name = 'moulavi_fine_tune'\n",
        "if wandb_active:\n",
        "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
        "train(dataset, model_moulavi, config, checkpoint_path=checkpoints_dir)\n",
        "if wandb_active:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "ro_AuceslEJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(config, device)\n",
        "chechkpoint = torch.load('/content/drive/MyDrive/NLP Class/checkpoints/model_checkpoint_1654448961.97679.pt',\n",
        "                         map_location=torch.device('cpu'))\n",
        "print(chechkpoint['epoch'])\n",
        "model.load_state_dict(chechkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "zvKgBqkQlJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eLjL4urM74r"
      },
      "outputs": [],
      "source": [
        "print('\\n'.join(predict(dataset, model, text='[BOM] توانا بود هر که')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2xm6xvwM74t"
      },
      "outputs": [],
      "source": [
        "# save torch model and configs\n",
        "import time\n",
        "\n",
        "torch.save({'model_state_dict': model.state_dict()}, f'../data/checkpoints/model_{time.time()}.pt')\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvNJf16WM74u"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 3. مدل بر پایه مکانیسم توجه </h3>\n",
        "    ابتدا به کمک کلاس SampleDataset پیش‌پردازش‌های لازم را انجام می‌دهیم که شامل خواندن متن و افزودن عبارات اول و پایان مصراع‌ها می‌باشد. هم‌چنین این کلاس تابعی برای بررسی قافیه‌دار بودن یک بیت دارد، بدین ترتیب که کلمات آخر دو مصراع آن را بررسی می‌کند و در صورتیکه دو حرف آخر یکسان داشته باشند، هم‌قافیه‌اند. <br>در تابع __getitem__ با دریافت ایندکس هر بیت، نتیجه‌ی encode شده‌ی آن بیت خروجی داده می‌شود که ورودی مناسب مدل است.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zXLEI0iJM74u"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "path_dir =  \"./data\"\n",
        "\n",
        "class SampleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            tokenizer,\n",
        "            base_path,\n",
        "            train_path,\n",
        "            max_epochs = 20,\n",
        "            batch_size = 256,\n",
        "            sequence_length = 6,\n",
        "            log_interval = 10\n",
        "    ): \n",
        "        self.start_mesra = '[BOM] '\n",
        "        self.end_mesra = '[EOM]'\n",
        "        self.start_beyt = ''\n",
        "        self.end_beyt = ' [EOS]'\n",
        "        self.base_path = base_path\n",
        "        self.train_path = train_path\n",
        "        self.sequence_length = sequence_length\n",
        "        self.beyts = self.load_beyts()\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def load_prepared_beyts(self):\n",
        "        with open(os.path.join(self.base_path, self.train_path)) as fp:\n",
        "            return fp.readlines()\n",
        "\n",
        "    def load_beyts(self):\n",
        "\n",
        "        beyt_file = []\n",
        "        with open(os.path.join(self.base_path, self.train_path)) as fp:\n",
        "            lines = fp.readlines()\n",
        "            for i in tqdm(range(0, len(lines) - 1, 2)):\n",
        "                mesra1 = self.start_mesra + lines[i].strip() + self.end_mesra\n",
        "                mesra2 = self.start_mesra + lines[i + 1].strip() + self.end_mesra\n",
        "                b = self.start_beyt + mesra1.strip() + ' ' + mesra2.strip() + self.end_beyt\n",
        "                beyt_file.append(b)\n",
        "        return beyt_file\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.beyts)\n",
        "\n",
        "    def does_ryhme(self, beyt):\n",
        "        mesras = beyt.split(self.end_mesra)\n",
        "        mesras = [x for x in filter(lambda x: len(x)>0, mesras)]\n",
        "        first_ghafie = [x for x in filter(lambda x: len(x)>0, mesras[0].split(\" \"))][-1]\n",
        "        second_ghafie = [x for x in filter(lambda x: len(x)>0, mesras[1].split(\" \"))][-1]\n",
        "        min_len = min(len(first_ghafie), len(second_ghafie))\n",
        "\n",
        "        for level in range(-1, -min_len-1, -1):\n",
        "            if not first_ghafie[level:] == second_ghafie[level:]:\n",
        "                break\n",
        "            if level < -1:\n",
        "                break\n",
        "        return level!=-1\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        current_beyt = self.beyts[index]\n",
        "        mesras = current_beyt.split(self.end_mesra)\n",
        "        mesras = [x for x in filter(lambda x: len(x)>0, mesras)]\n",
        "        first_token = self.tokenizer.encode(mesras[0])\n",
        "        second_token = self.tokenizer.encode(mesras[1])\n",
        "        # first_token.append(self.does_ryhme(current_beyt))\n",
        "        # second_token.append(self.does_ryhme(current_beyt))\n",
        "        tensors = (\n",
        "            first_token,\n",
        "            second_token\n",
        "        )\n",
        "        return self.tokenizer.encode(current_beyt.replace(self.end_mesra, \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"train.txt\"\n",
        "test_path = \"test.txt\"\n",
        "base_path = \"./data\"\n",
        "dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, train_path)\n",
        "val_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, test_path)"
      ],
      "metadata": {
        "id": "zLs4AvCZfYiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZmoED15M74w"
      },
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "     برای ساخت مدل بر پایه مکانیسم توجه، از مدل GPT2 بر پایه <a href=\"https://huggingface.co/bolbolzaban/gpt2-persian\">مدل pre_trained بلبل‌زبان</a> استفاده کردیم. توضیحات این مدل و دیتای مورد استفاده‌ی آن در <a href=\"https://khashei.medium.com/a-not-so-dangerous-ai-in-the-persian-language-39172a641c84\">این لینک</a> قابل مشاهده است. این مدل را بر اساس داده‌های مورد استفاده‌ی خودمان fine tune کردیم.<br> ابتدا این مدل گرفته می‌شود و متغیرهای لایه‌های درونی آن freeze می‌گردند، سپس داده‌ی اشعار پیش‌پردازش شده tokenize می‌شود و به دو بخش train و test تقسیم می‌گردد. برای train کردن، مدل بلبل‌زبان را بر اساس این داده fine tune می‌کنیم.<br>خروجی این مدل یک generator است که برای ساخت مصراع دوم از pipeline استفاده می‌کند که ورودی (مصراع اول) را به ساختاری مناسب ورودی دادن به مدل (با پیش‌پردازش و encoding) تبدیل کرده و به کمک مدل train شده، مصراع دوم را ساخته و decode می‌کند و بدین ترتیب بیت را می‌سازد.<br>کیفیت خروجی به تعداد epochها و سایز batchها بستگی خواهد داشت که افزایش آنها باعث بهبود نتیجه و کاهش سرعت training (بسته به سخت‌افزار سیستم) می‌گردد. ما پس از چندین آزمون و خطا با در نظر گرفتن مقدار 10 برای این دو متغیر به نتایج نسبتاً مناسب در زمان مطلوب رسیدیم.<br>کد این بخش روی gpu اجرا شده و در صورت نبودن gpu در سیستم، بخش مربوط به device در آن تغییر می‌یابد. مدل نهایی این بخش در <a href=\"url\">اینجا</a> قرار گرفته و قابل دانلود است.<br>برای اعمال رعایت یا عدم رعایت قافیه در این بخش، دو مدل مجزا از هم روی دو نمونه شعر با قافیه (شاهنامه) و بدون قافیه (غزلیات حافظ) train می‌شوند و با توجه به ورودی قافیه، نتیجه مدل مربوطه به عنوان خروجی نمایش داده می‌شود.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')"
      ],
      "metadata": {
        "id": "C9KPNRiafh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho5qoJduM74x"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel, \n",
        "    GPT2LMHeadModel,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "class MesraModel:\n",
        "    def __init__(self, train_path, test_path, model_dir=\"./model\"):\n",
        "        self.base_path = \"./data\"\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.generator = None\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.model_dir = model_dir\n",
        "        self.trainer = None\n",
        "        # self.cleaner = Cleaner()\n",
        "\n",
        "    def read_data(self, tokenizer, train_path=None, test_path=None):\n",
        "        train_path = train_path if train_path is not None else self.train_path\n",
        "        test_path = test_path if test_path is not None else self.test_path\n",
        "        train_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, train_path)\n",
        "        test_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, test_path)\n",
        "        # train_dataset = TextDataset(\n",
        "        #     tokenizer=tokenizer, file_path=os.path.join(self.base_path, train_path), block_size=128)\n",
        "        \n",
        "        # test_dataset = TextDataset(\n",
        "        #     tokenizer=tokenizer, file_path=os.path.join(self.base_path, test_path), block_size=128)\n",
        "        \n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "        return train_dataset, test_dataset, data_collator\n",
        "\n",
        "    def read_model(self, model_type='bolbolzaban/gpt2-persian'):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "        # self.tokenizer.add_tokens(['[EOM]', '[BOM]', '[EOS]'], special_tokens=True)\n",
        "        return self.model, self.tokenizer\n",
        "\n",
        "\n",
        "    def freeze_lower_layers(self):\n",
        "        for param in self.model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in (\n",
        "            self.model.base_model.h[23].parameters() or self.model.base_model.h[22].parameters()\n",
        "        ):\n",
        "            param.requires_grad = True\n",
        "            \n",
        "    def fine_tune_model(self, model, train_texts, val_texts, data_collator):\n",
        "        training_args = TrainingArguments(\n",
        "        output_dir=self.model_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=12,\n",
        "        # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "        # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "        per_device_train_batch_size=12,\n",
        "        per_device_eval_batch_size=12,\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        warmup_steps=500)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_texts,\n",
        "            eval_dataset=val_texts,\n",
        "        )\n",
        "        trainer.train()\n",
        "        self.trainer = trainer \n",
        "        return trainer\n",
        "    \n",
        "    def load_model(self):\n",
        "        model, tokenizer = self.read_model(self.model_dir)\n",
        "        generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256}, device=0)\n",
        "        self.generator = generator\n",
        "        return generator\n",
        "        \n",
        "    def init_generator(self, used_pretrained=False):      \n",
        "        if used_pretrained:\n",
        "            print(\"in init generator\")\n",
        "            model, tokenizer = self.read_model(self.model_dir)\n",
        "            print(\"read model successfully\")\n",
        "            model.to(torch.device(\"cuda\"))\n",
        "            print(\"convert model to cuda\")\n",
        "        else:\n",
        "            model, tokenizer = self.read_model()\n",
        "            \n",
        "        self.freeze_lower_layers()\n",
        "        train_texts, val_texts, data_collator = self.read_data(tokenizer)\n",
        "        trainer = self.fine_tune_model(model, train_texts, val_texts, data_collator)\n",
        "        model = trainer.model\n",
        "    \n",
        "        generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256}, device=0)\n",
        "        self.generator = generator\n",
        "        return generator\n",
        "\n",
        "    def save_model(self, dir=None):\n",
        "        dir = dir if dir is not None else self.model_dir\n",
        "        self.trainer.save_model(output_dir=dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "برای اعمال رعایت یا عدم رعایت قافیه در این بخش، دو مدل مجزا از هم روی دو نمونه شعر با قافیه (شاهنامه و مثنوی مولوی) و بدون قافیه (غزلیات حافظ و عراقی) train می‌شوند و با توجه به ورودی قافیه، نتیجه مدل مربوطه به عنوان خروجی نمایش داده می‌شود.\n",
        "</div>"
      ],
      "metadata": {
        "id": "SjCiXtPEhr1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MesraGenerator:\n",
        "    def __init__(self):\n",
        "        self.notryhme_poets = [\"hafez\", \"eraghi\"]\n",
        "        self.ryhme_poets = [\"ferdousi\", \"moulavi\"]\n",
        "        self.ryhme_model, self.notryhme_model = self.init_model()\n",
        "\n",
        "    def init_model(self):\n",
        "        mesra_model = MesraModel(\"train_path\", \"test_path\", model_dir=\"./notryhme_model\")\n",
        "        not_ryhme_generator = mesra_model.load_model()\n",
        "\n",
        "        mesra_model = MesraModel(\"train_path\", \"test_path\", model_dir=\"./model\")\n",
        "        ryhme_generator = mesra_model.load_model()\n",
        "        \n",
        "        return ryhme_generator, not_ryhme_generator\n",
        "\n",
        "    def train_all_poets(self):\n",
        "        ryhme_generator = None\n",
        "        not_ryhme_generator = None\n",
        "        for index, poet in enumerate(self.notryhme_poets):\n",
        "            train_path = f\"./data/{poet}_train.txt\"\n",
        "            test_path = f\"./data/{poet}_test.txt\"\n",
        "            mesra_model = MesraModel(train_path, test_path, model_dir=\"./notryhme_model\")\n",
        "            not_ryhme_generator = mesra_model.init_generator(used_pretrained=index!=0)\n",
        "            mesra_model.save_model(\"./notryhme_model\")\n",
        "\n",
        "        for index, poet in enumerate(self.ryhme_poets):\n",
        "            train_path = f\"./data/{poet}_train.txt\"\n",
        "            test_path = f\"./data/{poet}_test.txt\"\n",
        "            mesra_model = MesraModel(train_path, test_path, model_dir=\"./ryhme_model\")\n",
        "            ryhme_generator = mesra_model.init_generator(used_pretrained=index!=0)\n",
        "            mesra_model.save_model(\"./ryhme_model\")\n",
        "        return ryhme_generator, not_ryhme_generator \n",
        "    \n",
        "    def generate_mesra(self, first_mesra: str, has_ghafie: bool=True):\n",
        "        first_mesra = f\"[BOM] {first_mesra} [BOM]\"\n",
        "        if has_ghafie:\n",
        "            result = self.ryhme_model(first_mesra)\n",
        "        else:\n",
        "            result = self.notryhme_model(first_mesra)\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "O4N1R1XXgCei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yudQeuRKM740"
      },
      "outputs": [],
      "source": [
        "mesra_generator = MesraGenerator()\n",
        "mesra_generator.generate_mesra(\"[BOM]کاشت طره مولوی را در دل شب بی بهانه [BOM]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "از توابع زیر برای پیش‌پردازش و بازنویسی داده‌های train و test استفاده می‌گردد. هم‌چنین تابع save_not_ryhme_beyts می‌تواند فقط ابیاتی که قافیه ندارند را بیابد و ذخیره کند. این ابیات در ساخت مدل بر پایه ابیات بدون قافیه به ما کمک می‌کنند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "aB8d3czgHpBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns58_MmeM74z"
      },
      "outputs": [],
      "source": [
        "def does_ryhme(first_mesra, sec_mesra):\n",
        "    first_ghafie = [x for x in filter(lambda x: len(x)>0, first_mesra.split(\" \"))][-1]\n",
        "    second_ghafie = [x for x in filter(lambda x: len(x)>0, sec_mesra.split(\" \"))][-1]\n",
        "    min_len = min(len(first_ghafie), len(second_ghafie))\n",
        "\n",
        "    for level in range(-1, -min_len-1, -1):\n",
        "        if not first_ghafie[level:] == second_ghafie[level:]:\n",
        "            break\n",
        "        if level < -1:\n",
        "            break\n",
        "    return level!=-1\n",
        "\n",
        "    \n",
        "def write_to_file(lines, path):\n",
        "    with open(path, \"w\") as write_f:\n",
        "        for line in lines:\n",
        "            if len(line) > 1:\n",
        "                write_f.write(line)\n",
        "\n",
        "def save_not_ryhme_beyts(path):\n",
        "    out_path = \".\"+path.split(\".\")[-2]+\"_not_ryhme.txt\"\n",
        "    not_ryhme_lines = list()\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "        lines = list(filter(lambda x: len(x.strip())>1, lines))\n",
        "        for i in tqdm(range(0, len(lines) - 1, 2)):\n",
        "            mesra1 = lines[i].strip()\n",
        "            mesra2 = lines[i + 1].strip()\n",
        "            if not does_ryhme(mesra1, mesra2):\n",
        "                not_ryhme_lines.append(mesra1+os.linesep)\n",
        "                not_ryhme_lines.append(mesra2+os.linesep)\n",
        "    write_to_file(not_ryhme_lines, out_path)\n",
        "\n",
        "\n",
        "def split_filt_to_trian_test():\n",
        "    file_path = \"./data/eraghi_norm_not_ryhme.txt\"\n",
        "    train_perc = 0.8\n",
        "    with open(file_path) as f:\n",
        "        lines = f.readlines()\n",
        "        train_index = math.floor(len(lines)*train_perc)\n",
        "        train_index = train_index - (train_index%2)\n",
        "        train_lines = lines[:train_index]\n",
        "        test_lines = lines[train_index:]\n",
        "    write_to_file(train_lines, \"./data/eraghi_train.txt\")\n",
        "    write_to_file(test_lines, \"./data/eraghi_test.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_C7kZgcnM741"
      },
      "outputs": [],
      "source": [
        "split_filt_to_trian_test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_not_ryhme_beyts(\"./data/eraghi_norm.txt\")"
      ],
      "metadata": {
        "id": "wIlFYC9hIHFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}