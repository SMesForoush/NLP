{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir =  \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 132 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (4.63.1)\n",
      "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (1.4.2)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 825 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.5\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: packaging in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (0.7.0)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 87 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from datasets) (1.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from responses<0.19->datasets) (1.26.9)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[31mERROR: multiprocess 0.70.13 has requirement dill>=0.3.5.1, but you'll have dill 0.3.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyarrow, responses, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, dill, multiprocess, fsspec, xxhash, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.13 pyarrow-8.0.0 responses-0.18.0 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
    "# sample = generator('در یک اتفاق شگفت انگیز، پژوهشگران')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            base_path,\n",
    "            train_path,\n",
    "            max_epochs = 20,\n",
    "            batch_size = 256,\n",
    "            sequence_length = 6,\n",
    "            log_interval = 10\n",
    "    ): \n",
    "        self.start_mesra = '[BOM] '\n",
    "        self.end_mesra = '[EOM]'\n",
    "        self.start_beyt = ''\n",
    "        self.end_beyt = ' [EOS]'\n",
    "        self.base_path = base_path\n",
    "        self.train_path = train_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.beyts = self.load_beyts()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_prepared_beyts(self):\n",
    "        with open(os.path.join(self.base_path, self.train_path)) as fp:\n",
    "            return fp.readlines()\n",
    "\n",
    "    def load_beyts(self):\n",
    "\n",
    "        beyt_file = []\n",
    "        with open(os.path.join(self.base_path, self.train_path)) as fp:\n",
    "            lines = fp.readlines()\n",
    "            for i in tqdm(range(0, len(lines) - 1, 2)):\n",
    "                mesra1 = self.start_mesra + lines[i].strip() + self.end_mesra\n",
    "                mesra2 = self.start_mesra + lines[i + 1].strip() + self.end_mesra\n",
    "                b = self.start_beyt + mesra1.strip() + ' ' + mesra2.strip() + self.end_beyt\n",
    "                beyt_file.append(b)\n",
    "        return beyt_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.beyts)\n",
    "\n",
    "    def does_ryhme(self, beyt):\n",
    "        mesras = beyt.split(self.end_mesra)\n",
    "        mesras = [x for x in filter(lambda x: len(x)>0, mesras)]\n",
    "        first_ghafie = [x for x in filter(lambda x: len(x)>0, mesras[0].split(\" \"))][-1]\n",
    "        second_ghafie = [x for x in filter(lambda x: len(x)>0, mesras[1].split(\" \"))][-1]\n",
    "        min_len = min(len(first_ghafie), len(second_ghafie))\n",
    "\n",
    "        for level in range(-1, -min_len-1, -1):\n",
    "            if not first_ghafie[level:] == second_ghafie[level:]:\n",
    "                break\n",
    "            if level < -1:\n",
    "                break\n",
    "        return level!=-1\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        current_beyt = self.beyts[index]\n",
    "        mesras = current_beyt.split(self.end_mesra)\n",
    "        mesras = [x for x in filter(lambda x: len(x)>0, mesras)]\n",
    "        first_token = self.tokenizer.encode(mesras[0])\n",
    "        second_token = self.tokenizer.encode(mesras[1])\n",
    "        # first_token.append(self.does_ryhme(current_beyt))\n",
    "        # second_token.append(self.does_ryhme(current_beyt))\n",
    "        tensors = (\n",
    "            first_token,\n",
    "            second_token\n",
    "        )\n",
    "        return self.tokenizer.encode(current_beyt.replace(self.end_mesra, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 34725/34725 [00:00<00:00, 961862.92it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14883/14883 [00:00<00:00, 1007827.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n",
    "base_path = \"./data\"\n",
    "dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, train_path)\n",
    "val_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.does_ryhme(\"کسی کو بجوید همی تاج ویاه [EOM] خردباید ورای وگنج وسپاه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelWithLMHead,\n",
    "    BertModel, \n",
    "    GPT2LMHeadModel,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class MesraModel:\n",
    "    def __init__(self, train_path, test_path, model_dir=\"./model\"):\n",
    "        self.base_path = \"./data\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.generator = None\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.model_dir = model_dir\n",
    "        self.trainer = None\n",
    "        # self.cleaner = Cleaner()\n",
    "\n",
    "    def read_data(self, tokenizer, train_path=None, test_path=None):\n",
    "        train_path = train_path if train_path is not None else self.train_path\n",
    "        test_path = test_path if test_path is not None else self.test_path\n",
    "        train_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, train_path)\n",
    "        test_dataset = SampleDataset(AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian'), base_path, test_path)\n",
    "        # train_dataset = TextDataset(\n",
    "        #     tokenizer=tokenizer, file_path=os.path.join(self.base_path, train_path), block_size=128)\n",
    "        \n",
    "        # test_dataset = TextDataset(\n",
    "        #     tokenizer=tokenizer, file_path=os.path.join(self.base_path, test_path), block_size=128)\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        return train_dataset, test_dataset, data_collator\n",
    "\n",
    "    def read_model(self, model_type='bolbolzaban/gpt2-persian'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "        # self.tokenizer.add_tokens(['[EOM]', '[BOM]', '[EOS]'], special_tokens=True)\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "\n",
    "    def freeze_lower_layers(self):\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in (\n",
    "            self.model.base_model.h[23].parameters() or self.model.base_model.h[22].parameters()\n",
    "        ):\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def fine_tune_model(self, model, train_texts, val_texts, data_collator):\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=self.model_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=12,\n",
    "        # Set the batch size to a maximum value that could fit into GPU memory,\n",
    "        # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size=12,\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000,\n",
    "        warmup_steps=500)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_texts,\n",
    "            eval_dataset=val_texts,\n",
    "        )\n",
    "        trainer.train()\n",
    "        self.trainer = trainer \n",
    "        return trainer\n",
    "    \n",
    "    def load_model(self):\n",
    "        model, tokenizer = self.read_model(self.model_dir)\n",
    "        generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256}, device=0)\n",
    "        self.generator = generator\n",
    "        return generator\n",
    "        \n",
    "    def init_generator(self, used_pretrained=False):      \n",
    "        if used_pretrained:\n",
    "            print(\"in init generator\")\n",
    "            model, tokenizer = self.read_model(self.model_dir)\n",
    "            print(\"read model successfully\")\n",
    "            model.to(torch.device(\"cuda\"))\n",
    "            print(\"convert model to cuda\")\n",
    "        else:\n",
    "            model, tokenizer = self.read_model()\n",
    "            \n",
    "        self.freeze_lower_layers()\n",
    "        train_texts, val_texts, data_collator = self.read_data(tokenizer)\n",
    "        trainer = self.fine_tune_model(model, train_texts, val_texts, data_collator)\n",
    "        model = trainer.model\n",
    "    \n",
    "        generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256}, device=0)\n",
    "        self.generator = generator\n",
    "        return generator\n",
    "\n",
    "    def save_model(self, dir=None):\n",
    "        dir = dir if dir is not None else self.model_dir\n",
    "        self.trainer.save_model(output_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesraGenerator:\n",
    "    def __init__(self):\n",
    "        self.notryhme_poets = [\"hafez\", \"eraghi\"]\n",
    "        self.ryhme_poets = [\"ferdousi\", \"moulavi\"]\n",
    "        self.ryhme_model, self.notryhme_model = self.init_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        mesra_model = MesraModel(\"train_path\", \"test_path\", model_dir=\"./notryhme_model\")\n",
    "        not_ryhme_generator = mesra_model.load_model()\n",
    "\n",
    "        mesra_model = MesraModel(\"train_path\", \"test_path\", model_dir=\"./model\")\n",
    "        ryhme_generator = mesra_model.load_model()\n",
    "        \n",
    "        return ryhme_generator, not_ryhme_generator\n",
    "\n",
    "    def train_all_poets(self):\n",
    "        ryhme_generator = None\n",
    "        not_ryhme_generator = None\n",
    "        for index, poet in enumerate(self.notryhme_poets):\n",
    "            train_path = f\"./data/{poet}_train.txt\"\n",
    "            test_path = f\"./data/{poet}_test.txt\"\n",
    "            mesra_model = MesraModel(train_path, test_path, model_dir=\"./notryhme_model\")\n",
    "            not_ryhme_generator = mesra_model.init_generator(used_pretrained=index!=0)\n",
    "            mesra_model.save_model(\"./notryhme_model\")\n",
    "\n",
    "        for index, poet in enumerate(self.ryhme_poets):\n",
    "            train_path = f\"./data/{poet}_train.txt\"\n",
    "            test_path = f\"./data/{poet}_test.txt\"\n",
    "            mesra_model = MesraModel(train_path, test_path, model_dir=\"./ryhme_model\")\n",
    "            ryhme_generator = mesra_model.init_generator(used_pretrained=index!=0)\n",
    "            mesra_model.save_model(\"./ryhme_model\")\n",
    "        return ryhme_generator, not_ryhme_generator \n",
    "    \n",
    "    def generate_mesra(self, first_mesra: str, has_ghafie: bool=True):\n",
    "        first_mesra = f\"[BOM] {first_mesra} [BOM]\"\n",
    "        if has_ghafie:\n",
    "            result = self.ryhme_model(first_mesra)\n",
    "        else:\n",
    "            result = self.notryhme_model(first_mesra)\n",
    "\n",
    "        return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[BOM] [BOM]کاشت طره مولوی را در دل شب بی بهانه [BOM] [BOM] جز به دست آن سرزلف معنبر نمی'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_generator = MesraGenerator()\n",
    "mesra_generator.generate_mesra(\"[BOM]کاشت طره مولوی را در دل شب بی بهانه [BOM]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[BOM] [BOM]کاشت طره مولوی را در دل شب بی بهانه [BOM] [BOM] با دل مجروح ما کرد آشنا با یک بهانه'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_generator.generate_mesra(\"[BOM]کاشت طره مولوی را در دل شب بی بهانه [BOM]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def does_ryhme(first_mesra, sec_mesra):\n",
    "    first_ghafie = [x for x in filter(lambda x: len(x)>0, first_mesra.split(\" \"))][-1]\n",
    "    second_ghafie = [x for x in filter(lambda x: len(x)>0, sec_mesra.split(\" \"))][-1]\n",
    "    min_len = min(len(first_ghafie), len(second_ghafie))\n",
    "\n",
    "    for level in range(-1, -min_len-1, -1):\n",
    "        if not first_ghafie[level:] == second_ghafie[level:]:\n",
    "            break\n",
    "        if level < -1:\n",
    "            break\n",
    "    return level!=-1\n",
    "\n",
    "    \n",
    "def write_to_file(lines, path):\n",
    "    with open(path, \"w\") as write_f:\n",
    "        for line in lines:\n",
    "            if len(line) > 1:\n",
    "                write_f.write(line)\n",
    "\n",
    "def save_not_ryhme_beyts(path):\n",
    "    out_path = \".\"+path.split(\".\")[-2]+\"_not_ryhme.txt\"\n",
    "    not_ryhme_lines = list()\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = list(filter(lambda x: len(x.strip())>1, lines))\n",
    "        for i in tqdm(range(0, len(lines) - 1, 2)):\n",
    "            mesra1 = lines[i].strip()\n",
    "            mesra2 = lines[i + 1].strip()\n",
    "            if not does_ryhme(mesra1, mesra2):\n",
    "                not_ryhme_lines.append(mesra1+os.linesep)\n",
    "                not_ryhme_lines.append(mesra2+os.linesep)\n",
    "    write_to_file(not_ryhme_lines, out_path)\n",
    "\n",
    "\n",
    "def split_filt_to_trian_test():\n",
    "    file_path = \"./data/eraghi_norm_not_ryhme.txt\"\n",
    "    train_perc = 0.8\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        train_index = math.floor(len(lines)*train_perc)\n",
    "        train_index = train_index - (train_index%2)\n",
    "        train_lines = lines[:train_index]\n",
    "        test_lines = lines[train_index:]\n",
    "    write_to_file(train_lines, \"./data/eraghi_train.txt\")\n",
    "    write_to_file(test_lines, \"./data/eraghi_test.txt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_filt_to_trian_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5726/5726 [00:00<00:00, 248346.38it/s]\n"
     ]
    }
   ],
   "source": [
    "save_not_ryhme_beyts(\"./data/eraghi_norm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
