{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33msoroushtabesh\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/soroush/PycharmProjects/NLP/hw3/wandb/run-20220604_214452-38pzbi2o</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o\" target=\"_blank\">dark-butterfly-5</a></strong> to <a href=\"https://wandb.ai/soroushtabesh/ferdousi-generator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7fd4dd24b190>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ferdousi-generator\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "config.max_epochs = 20\n",
    "config.batch_size = 256\n",
    "config.sequence_length = 6\n",
    "config.log_interval = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.embedding_dim = 256\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).cuda(),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            args,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        with open('../data/ferdousi_norm.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = ['__bom__ ' + line if i % 2 == 0 else '__bos__ ' + line for i, line in\n",
    "                 enumerate(lines)]\n",
    "        words = [word for line in lines for word in line.split()]\n",
    "        return words\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.args.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensors = (\n",
    "            torch.tensor(self.words_indexes[index:index + self.args.sequence_length]).cuda(),\n",
    "            torch.tensor(self.words_indexes[index + 1:index + self.args.sequence_length + 1]).cuda(),\n",
    "        )\n",
    "        return tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(dataset, model, args):\n",
    "    wandb.watch(model)\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print({'batch_count': len(dataloader), 'epoch_count': args.max_epochs})\n",
    "    for epoch in range(args.max_epochs):\n",
    "        state_h, state_c = model.init_state(args.sequence_length)\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'epoch': epoch, 'batch': batch, 'loss': loss.item()})\n",
    "            if batch % args.log_interval == 0:\n",
    "                wandb.log({\"loss\": loss})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).cuda()\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  0,   3,  81, 363, 118,   2], device='cuda:0'), tensor([  3,  81, 363, 118,   2,  98], device='cuda:0'))\n",
      "(tensor([  3,  81, 363, 118,   2,  98], device='cuda:0'), tensor([ 81, 363, 118,   2,  98,   1], device='cuda:0'))\n",
      "(tensor([ 81, 363, 118,   2,  98,   1], device='cuda:0'), tensor([363, 118,   2,  98,   1, 365], device='cuda:0'))\n",
      "(tensor([363, 118,   2,  98,   1, 365], device='cuda:0'), tensor([118,   2,  98,   1, 365, 698], device='cuda:0'))\n",
      "(tensor([118,   2,  98,   1, 365, 698], device='cuda:0'), tensor([  2,  98,   1, 365, 698, 221], device='cuda:0'))\n",
      "(tensor([  2,  98,   1, 365, 698, 221], device='cuda:0'), tensor([  98,    1,  365,  698,  221, 3552], device='cuda:0'))\n",
      "(tensor([  98,    1,  365,  698,  221, 3552], device='cuda:0'), tensor([   1,  365,  698,  221, 3552,    0], device='cuda:0'))\n",
      "(tensor([   1,  365,  698,  221, 3552,    0], device='cuda:0'), tensor([ 365,  698,  221, 3552,    0,  363], device='cuda:0'))\n",
      "(tensor([ 365,  698,  221, 3552,    0,  363], device='cuda:0'), tensor([ 698,  221, 3552,    0,  363,   81], device='cuda:0'))\n",
      "(tensor([ 698,  221, 3552,    0,  363,   81], device='cuda:0'), tensor([ 221, 3552,    0,  363,   81,    2], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(config)\n",
    "# get first 10 items in dataset\n",
    "for i in range(10):\n",
    "    print(dataset[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'batch': 363, 'loss': 4.209418773651123}\n",
      "{'epoch': 7, 'batch': 364, 'loss': 3.847109079360962}\n",
      "{'epoch': 7, 'batch': 365, 'loss': 3.8111469745635986}\n",
      "{'epoch': 7, 'batch': 366, 'loss': 3.6310434341430664}\n",
      "{'epoch': 7, 'batch': 367, 'loss': 3.771148443222046}\n",
      "{'epoch': 7, 'batch': 368, 'loss': 3.8145344257354736}\n",
      "{'epoch': 7, 'batch': 369, 'loss': 3.7349956035614014}\n",
      "{'epoch': 7, 'batch': 370, 'loss': 3.785506248474121}\n",
      "{'epoch': 7, 'batch': 371, 'loss': 3.9743564128875732}\n",
      "{'epoch': 7, 'batch': 372, 'loss': 3.65920090675354}\n",
      "{'epoch': 7, 'batch': 373, 'loss': 3.840552568435669}\n",
      "{'epoch': 7, 'batch': 374, 'loss': 3.857278823852539}\n",
      "{'epoch': 7, 'batch': 375, 'loss': 3.4766480922698975}\n",
      "{'epoch': 7, 'batch': 376, 'loss': 3.699756622314453}\n",
      "{'epoch': 7, 'batch': 377, 'loss': 3.8779518604278564}\n",
      "{'epoch': 7, 'batch': 378, 'loss': 3.964479684829712}\n",
      "{'epoch': 7, 'batch': 379, 'loss': 3.995656728744507}\n",
      "{'epoch': 7, 'batch': 380, 'loss': 3.993419647216797}\n",
      "{'epoch': 7, 'batch': 381, 'loss': 3.7229907512664795}\n",
      "{'epoch': 7, 'batch': 382, 'loss': 4.077213764190674}\n",
      "{'epoch': 7, 'batch': 383, 'loss': 4.228193759918213}\n",
      "{'epoch': 7, 'batch': 384, 'loss': 3.9997284412384033}\n",
      "{'epoch': 7, 'batch': 385, 'loss': 3.815762519836426}\n",
      "{'epoch': 7, 'batch': 386, 'loss': 4.038028240203857}\n",
      "{'epoch': 7, 'batch': 387, 'loss': 3.581042528152466}\n",
      "{'epoch': 7, 'batch': 388, 'loss': 3.5108087062835693}\n",
      "{'epoch': 7, 'batch': 389, 'loss': 3.9503695964813232}\n",
      "{'epoch': 7, 'batch': 390, 'loss': 3.80790114402771}\n",
      "{'epoch': 7, 'batch': 391, 'loss': 3.836116075515747}\n",
      "{'epoch': 7, 'batch': 392, 'loss': 3.5553112030029297}\n",
      "{'epoch': 7, 'batch': 393, 'loss': 3.8614423274993896}\n",
      "{'epoch': 7, 'batch': 394, 'loss': 3.7185728549957275}\n",
      "{'epoch': 7, 'batch': 395, 'loss': 3.8141231536865234}\n",
      "{'epoch': 7, 'batch': 396, 'loss': 3.8515424728393555}\n",
      "{'epoch': 7, 'batch': 397, 'loss': 3.8173093795776367}\n",
      "{'epoch': 7, 'batch': 398, 'loss': 3.7610368728637695}\n",
      "{'epoch': 7, 'batch': 399, 'loss': 3.499333620071411}\n",
      "{'epoch': 7, 'batch': 400, 'loss': 3.746126413345337}\n",
      "{'epoch': 7, 'batch': 401, 'loss': 3.730260133743286}\n",
      "{'epoch': 7, 'batch': 402, 'loss': 3.9946439266204834}\n",
      "{'epoch': 7, 'batch': 403, 'loss': 4.26870584487915}\n",
      "{'epoch': 7, 'batch': 404, 'loss': 3.903212785720825}\n",
      "{'epoch': 7, 'batch': 405, 'loss': 4.04760217666626}\n",
      "{'epoch': 7, 'batch': 406, 'loss': 4.121025562286377}\n",
      "{'epoch': 7, 'batch': 407, 'loss': 3.635011911392212}\n",
      "{'epoch': 7, 'batch': 408, 'loss': 4.290930271148682}\n",
      "{'epoch': 7, 'batch': 409, 'loss': 3.6557819843292236}\n",
      "{'epoch': 7, 'batch': 410, 'loss': 3.797571897506714}\n",
      "{'epoch': 7, 'batch': 411, 'loss': 3.8102121353149414}\n",
      "{'epoch': 7, 'batch': 412, 'loss': 3.7718076705932617}\n",
      "{'epoch': 7, 'batch': 413, 'loss': 3.819549322128296}\n",
      "{'epoch': 7, 'batch': 414, 'loss': 3.722973108291626}\n",
      "{'epoch': 7, 'batch': 415, 'loss': 3.563998222351074}\n",
      "{'epoch': 7, 'batch': 416, 'loss': 3.8162758350372314}\n",
      "{'epoch': 7, 'batch': 417, 'loss': 3.9047155380249023}\n",
      "{'epoch': 7, 'batch': 418, 'loss': 3.835650682449341}\n",
      "{'epoch': 7, 'batch': 419, 'loss': 3.8135597705841064}\n",
      "{'epoch': 7, 'batch': 420, 'loss': 3.835928201675415}\n",
      "{'epoch': 7, 'batch': 421, 'loss': 4.0709452629089355}\n",
      "{'epoch': 7, 'batch': 422, 'loss': 3.8334226608276367}\n",
      "{'epoch': 7, 'batch': 423, 'loss': 3.7544214725494385}\n",
      "{'epoch': 7, 'batch': 424, 'loss': 4.066383361816406}\n",
      "{'epoch': 7, 'batch': 425, 'loss': 3.823296308517456}\n",
      "{'epoch': 7, 'batch': 426, 'loss': 3.6314079761505127}\n",
      "{'epoch': 7, 'batch': 427, 'loss': 3.558120012283325}\n",
      "{'epoch': 7, 'batch': 428, 'loss': 3.669792413711548}\n",
      "{'epoch': 7, 'batch': 429, 'loss': 3.749802589416504}\n",
      "{'epoch': 7, 'batch': 430, 'loss': 3.406395196914673}\n",
      "{'epoch': 7, 'batch': 431, 'loss': 3.7276480197906494}\n",
      "{'epoch': 7, 'batch': 432, 'loss': 3.8900604248046875}\n",
      "{'epoch': 7, 'batch': 433, 'loss': 3.9515583515167236}\n",
      "{'epoch': 7, 'batch': 434, 'loss': 3.7028608322143555}\n",
      "{'epoch': 7, 'batch': 435, 'loss': 3.7786169052124023}\n",
      "{'epoch': 7, 'batch': 436, 'loss': 3.833015203475952}\n",
      "{'epoch': 7, 'batch': 437, 'loss': 3.9043824672698975}\n",
      "{'epoch': 7, 'batch': 438, 'loss': 3.8864104747772217}\n",
      "{'epoch': 7, 'batch': 439, 'loss': 3.6018800735473633}\n",
      "{'epoch': 7, 'batch': 440, 'loss': 3.769864797592163}\n",
      "{'epoch': 7, 'batch': 441, 'loss': 3.3600080013275146}\n",
      "{'epoch': 7, 'batch': 442, 'loss': 3.999546766281128}\n",
      "{'epoch': 7, 'batch': 443, 'loss': 3.8988237380981445}\n",
      "{'epoch': 7, 'batch': 444, 'loss': 3.7751567363739014}\n",
      "{'epoch': 7, 'batch': 445, 'loss': 3.8429691791534424}\n",
      "{'epoch': 7, 'batch': 446, 'loss': 3.743973970413208}\n",
      "{'epoch': 7, 'batch': 447, 'loss': 3.783724546432495}\n",
      "{'epoch': 7, 'batch': 448, 'loss': 3.842731475830078}\n",
      "{'epoch': 7, 'batch': 449, 'loss': 3.7592971324920654}\n",
      "{'epoch': 7, 'batch': 450, 'loss': 3.81467604637146}\n",
      "{'epoch': 7, 'batch': 451, 'loss': 3.6967933177948}\n",
      "{'epoch': 7, 'batch': 452, 'loss': 3.7236013412475586}\n",
      "{'epoch': 7, 'batch': 453, 'loss': 3.9935874938964844}\n",
      "{'epoch': 7, 'batch': 454, 'loss': 3.805300712585449}\n",
      "{'epoch': 7, 'batch': 455, 'loss': 4.048088550567627}\n",
      "{'epoch': 7, 'batch': 456, 'loss': 3.619264602661133}\n",
      "{'epoch': 7, 'batch': 457, 'loss': 3.65619158744812}\n",
      "{'epoch': 7, 'batch': 458, 'loss': 4.132577419281006}\n",
      "{'epoch': 7, 'batch': 459, 'loss': 3.8072774410247803}\n",
      "{'epoch': 7, 'batch': 460, 'loss': 3.8970396518707275}\n",
      "{'epoch': 7, 'batch': 461, 'loss': 3.35860276222229}\n",
      "{'epoch': 7, 'batch': 462, 'loss': 3.919743537902832}\n",
      "{'epoch': 7, 'batch': 463, 'loss': 3.6480157375335693}\n",
      "{'epoch': 7, 'batch': 464, 'loss': 3.706388473510742}\n",
      "{'epoch': 7, 'batch': 465, 'loss': 3.6733179092407227}\n",
      "{'epoch': 7, 'batch': 466, 'loss': 3.598482370376587}\n",
      "{'epoch': 7, 'batch': 467, 'loss': 3.7621166706085205}\n",
      "{'epoch': 7, 'batch': 468, 'loss': 3.8957221508026123}\n",
      "{'epoch': 7, 'batch': 469, 'loss': 4.206190586090088}\n",
      "{'epoch': 7, 'batch': 470, 'loss': 4.097470760345459}\n",
      "{'epoch': 7, 'batch': 471, 'loss': 4.006556987762451}\n",
      "{'epoch': 7, 'batch': 472, 'loss': 3.9739768505096436}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7063/2542580308.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_7063/1670458408.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(dataset, model, args)\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0mstate_c\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstate_c\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'epoch'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'batch'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'loss'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    130\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    133\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(dataset)\n",
    "\n",
    "train(dataset, model, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__bom__\n",
      "توانا\n",
      "بود\n",
      "هر\n",
      "که\n",
      "دریا\n",
      "سپاس\n",
      "خاک\n",
      "و\n",
      "دل\n",
      "__bos__\n",
      "وزو\n",
      "سر\n",
      "بزرگی\n",
      "دلی\n",
      "را\n",
      "کسی\n",
      "__bom__\n",
      "فدای\n",
      "تو\n",
      "روشن\n",
      "نه\n",
      "نیکوترست\n",
      "__bos__\n",
      "سواران\n",
      "بدخواه\n",
      "سر\n",
      "نگسلی\n",
      "__bom__\n",
      "نخواهم\n",
      "که\n",
      "بیند\n",
      "بدین\n",
      "داستان\n",
      "__bos__\n",
      "نگیرم\n",
      "به\n",
      "جز\n",
      "دل\n",
      "دلارای\n",
      "بود\n",
      "__bom__\n",
      "سیاوش\n",
      "سر\n",
      "شاه\n",
      "بر\n",
      "چشم\n",
      "شاه\n",
      "__bos__\n",
      "بیاراستن\n",
      "بیکران\n",
      "با\n",
      "سپاه\n",
      "__bom__\n",
      "گر\n",
      "ایدونک\n",
      "هم\n",
      "بار\n",
      "بشتافتم\n",
      "__bos__\n",
      "تو\n",
      "گویی\n",
      "جوانست\n",
      "و\n",
      "هوش\n",
      "اندر\n",
      "آی\n",
      "__bom__\n",
      "نپذرفت\n",
      "با\n",
      "من\n",
      "برو\n",
      "زر\n",
      "به\n",
      "هم\n",
      "__bos__\n",
      "ز\n",
      "فرمان\n",
      "ما\n",
      "برتر\n",
      "از\n",
      "شهریار\n",
      "__bom__\n",
      "سیاووش\n",
      "پردانش\n",
      "و\n",
      "چون\n",
      "شیر\n",
      "بدار\n",
      "__bos__\n",
      "تو\n",
      "باشد\n",
      "ز\n",
      "ما\n",
      "چون\n",
      "ترا\n",
      "بازگرد\n",
      "__bom__\n",
      "یکی\n",
      "سان\n",
      "که\n",
      "راند\n",
      "چنین\n",
      "نابکار\n",
      "__bos__\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n'.join(predict(dataset, model, text='__bom__ توانا بود هر که')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# save torch model and configs\n",
    "import time\n",
    "torch.save({'model_state_dict':model.state_dict()}, f'../data/checkpoints/model_{time.time()}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27a225e506849d1bdf80008d630248f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▆▅▅▆▅▃▃▄▃▃▃▃▂▃▃▃▃▃▃▃▃▂▁▃▃▂▂▂▂▂▂▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>4.09747</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">dark-butterfly-5</strong>: <a href=\"https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o\" target=\"_blank\">https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220604_214452-38pzbi2o/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}