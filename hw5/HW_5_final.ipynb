{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRkhF2-fSce1"
      },
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        تمرین پنجم درس پردازش زبان‌های طبیعی\n",
        "    </h1>\n",
        "    <h3>\n",
        "        گردآورندگان:<br/>\n",
        "        ساحل مس‌فروش، سروش تابش، درنا دهقانی\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    ما در این تمرین ترک \"استخراج موجودیت های نام دار فارسی بر روی دادگان خبری\" را انتخاب کردیم. در این تمرین قصد داریم به کمک داده‌های برچسب‌گذاری‌شده، موجودیت‌های نام‌دار را در یک متن خبری استخراج کنیم؛ از جمله نام افراد، مکان‌ها، تاریخ‌ها و ... . برای این کار از یک مدل از پیش‌آموزش ‌دیده‌شده با مکانیزم توجه (پارس برت آزمایشگاه هوشواره) استفاده کردیم. \n",
        "</div>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h4> پیش پردازش </h4>\n",
        "    در این بخش به کمک تابع <code>convert_to_token</code> ابتدا داده‌ی داده شده را دریافت کردیم. سپس آنها را tokenize کردیم و به هر token متناسب با موجودیت نام‌دارش برچسبی اختصاص دادیم؛ بدین صورت که اگر بخشی از موجودیت نام‌دار <code>label</code> بود، اولین token آن برچسب <code>B_label</code> و بقیه tokenهای آن موجودیت نام‌دار برچسب <code>I_label</code> می‌گیرد و اگر هیچ موجودیت نام‌داری نداشت، برچسب <code>O</code> می‌گیرد. هم‌چنین tokenهایی که UNK تشخیص داده می‌شوند را حذف کردیم.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhnppEKPSce4",
        "outputId": "407884c9-d6df-42c1-c635-79e271c77d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bLuosBIBSce5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ob-ZdAIqSce6"
      },
      "outputs": [],
      "source": [
        "def convert_to_token(text, annotations, tokenizer):\n",
        "    model_name = 'HooshvareLab/bert-base-parsbert-armanner-uncased'\n",
        "    result = list()\n",
        "    classified_tokens = set()\n",
        "\n",
        "    for annot in annotations:\n",
        "        ner_range = annot.get(\"range\")\n",
        "        ner_name = annot.get(\"name\")\n",
        "        ner_text = text[ner_range[0]:ner_range[1]]\n",
        "        ner_tokens = tokenizer.tokenize(ner_text)\n",
        "        for i, token in enumerate(ner_tokens):\n",
        "            if i == 0:\n",
        "                result.append({\"Token\": token, \"NER_label\": f\"B_{ner_name}\"})\n",
        "            else:\n",
        "                result.append({\"Token\": token, \"NER_label\": f\"I_{ner_name}\"})\n",
        "            classified_tokens.add(token)\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    for token in tokenized_text:\n",
        "        if not token in classified_tokens:\n",
        "            result.append({\"Token\": token, \"NER_label\": f\"O\"})\n",
        "    return result\n",
        "\n",
        "\n",
        "datapath = \"dataset_annotated_splited.json\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "data = json.load(open(datapath))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyK_JqpKSce6"
      },
      "source": [
        "<div dir=\"rtl\" style ='font-family: \"B Nazanin\";'>\n",
        "    سپس تابع فوق را بر روی داده‌های train، eval و test صدا زدیم و آن را روی تیتر و متن اخبار اعمال کردیم و تا لیستی از تمامی tokenها و برچسب‌هایشان داشته باشیم و آن‌ها را در قالب csv ذخیره کردیم.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WgSNiWqoSce6"
      },
      "outputs": [],
      "source": [
        "dataset_parts = ['train', 'eval', 'test']\n",
        "\n",
        "for dp in dataset_parts:\n",
        "    temp_list = []\n",
        "    length = len(data.get(dp))\n",
        "    for i in range(length):\n",
        "        parts = ['header', 'text']\n",
        "        for p in parts:\n",
        "            t = convert_to_token(data.get(dp)[i].get(p), data.get(dp)[i].get(\"annotations\")[0].get(p), tokenizer)\n",
        "            temp_list += [j for j in t if not (j['Token'] == '[UNK]')]\n",
        "    temp_df = pd.DataFrame(temp_list)\n",
        "    temp_df.to_csv(f'{dp}_labels.csv', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\" style ='font-family: \"B Nazanin\";'>\n",
        "    به منظور بهبود مدل، از داده‌های موجود در <a href='https://github.com/nasrin-taghizadeh/NSURL-Persian-NER'>این لینک</a> استفاده کردیم. بدین صورت که داده‌های موجود در بخش 300k را با هم ادغام کرده و ذخیره کردیم تا در مراحل بعد استفاده کنیم. ساختار ذخیره این داده‌ها مشابه بخش قبل است. برخی کلمات در این داده شامل چند token بودند که آنها را ادغام کردیم و در قالب یک token درآوردیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "oqjAllBJVDwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = range(1, 710)\n",
        "\n",
        "lines = []\n",
        "for file in file_list:\n",
        "    with open(f'300K/{file}.txt', encoding='utf-8') as f:\n",
        "        file_lines = [line.rstrip().split() for line in f if line.rstrip()]\n",
        "        lines += file_lines"
      ],
      "metadata": {
        "id": "SZHmXI89V-C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lines)):\n",
        "    if len(lines[i]) > 2:\n",
        "        word = ' '.join(lines[i][0:-1])\n",
        "        lines[i] = [word, lines[i][-1]]\n",
        "\n",
        "df = pd.DataFrame(lines, columns=['Token', 'NER_label'])\n",
        "df.to_csv('300k_complete.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "egeZlB0jWaMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "HW 5 final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}