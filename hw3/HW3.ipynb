{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2faf6d",
   "metadata": {},
   "source": [
    "\n",
    "</h1> <h1 style='direction:rtl; font-family: \"B Lotus\";'> ØªÙ…Ø±ÛŒÙ† Ø¯ÙˆÙ… - Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¹Ø±Ù‡Ø§ÛŒ Ø¨Ø§ Ù„Ù‡Ø¬Ù‡â€ŒÛŒ Ø´ÛŒØ±Ø§Ø²ÛŒ\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d3ef6",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;font-family: \"B Lotus\";'> Ø¯Ø§Ø¯Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ </h2> \n",
    "\n",
    "<p style='direction:rtl;font-family: \"B Lotus\";' >\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3251a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"./data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4fe8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: hazm in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: six in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Requirement already satisfied: transformers in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: requests in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Collecting torch\n",
      "  Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/sahel/university/NLP/.venv/lib/python3.8/site-packages (from torch) (4.2.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install hazm\n",
    "! pip install transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab86f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1902c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    def __init__(self):\n",
    "        self.normalizer = Normalizer()\n",
    "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
    "        self.file_range = None\n",
    "\n",
    "\n",
    "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  \n",
    "        file_range = range(*file_range) if file_range is not None else None \n",
    "        file_names = glob.glob(file_patterns)\n",
    "        mesra_collection = []\n",
    "        for file_name in file_names:\n",
    "            if normalize:\n",
    "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
    "            else:\n",
    "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
    "        return mesra_collection\n",
    "    \n",
    "\n",
    "    def read_documents(self):\n",
    "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
    "        cleaned = self.clean_data(poems)\n",
    "        return ' \\n '.join(cleaned)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b0bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99217\n"
     ]
    }
   ],
   "source": [
    "path=f\"{path_dir}/ferdousi.txt\"\n",
    "cleaner = Cleaner()\n",
    "mesras = cleaner.read_poems(file_patterns=path, file_range=None, normalize=False)\n",
    "print(len(mesras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5e8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 452M/452M [03:31<00:00, 2.23MB/s]   \n",
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ù…Ø§',\n",
       " 'Ø¯Ø±',\n",
       " 'Ù‡ÙˆØ´',\n",
       " '[ZWNJ]',\n",
       " 'ÙˆØ§Ø±Ù‡',\n",
       " 'Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ…',\n",
       " 'Ø¨Ø§',\n",
       " 'Ø§Ù†ØªÙ‚Ø§Ù„',\n",
       " 'ØµØ­ÛŒØ­',\n",
       " 'Ø¯Ø§Ù†Ø´',\n",
       " 'Ùˆ',\n",
       " 'Ø¢',\n",
       " '##Ú¯Ø§Ù‡ÛŒ',\n",
       " 'ØŒ',\n",
       " 'Ù‡Ù…Ù‡',\n",
       " 'Ø§ÙØ±Ø§Ø¯',\n",
       " 'Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯',\n",
       " 'Ø§Ø²',\n",
       " 'Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ',\n",
       " 'Ù‡ÙˆØ´Ù…Ù†Ø¯',\n",
       " 'Ø§Ø³ØªÙØ§Ø¯Ù‡',\n",
       " 'Ú©Ù†Ù†Ø¯',\n",
       " '.',\n",
       " 'Ø´Ø¹Ø§Ø±',\n",
       " 'Ù…Ø§',\n",
       " 'Ù‡ÙˆØ´',\n",
       " 'Ù…ØµÙ†ÙˆØ¹ÛŒ',\n",
       " 'Ø¨Ø±Ø§ÛŒ',\n",
       " 'Ù‡Ù…Ù‡',\n",
       " 'Ø§Ø³Øª',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "# v3.0\n",
    "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "text = \"Ù…Ø§ Ø¯Ø± Ù‡ÙˆØ´â€ŒÙˆØ§Ø±Ù‡ Ù…Ø¹ØªÙ‚Ø¯ÛŒÙ… Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ ØµØ­ÛŒØ­ Ø¯Ø§Ù†Ø´ Ùˆ Ø¢Ú¯Ø§Ù‡ÛŒØŒ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯. Ø´Ø¹Ø§Ø± Ù…Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø³Øª.\"\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b7dd04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahel/university/NLP/.venv/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /home/sahel/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"beta1\": 0.9,\n",
      "  \"beta2\": 0.98,\n",
      "  \"bos_token_id\": 8,\n",
      "  \"data_path\": \"\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"embed_dropout\": 0.1,\n",
      "  \"eos_token_id\": 9,\n",
      "  \"epsilon\": 1e-09,\n",
      "  \"eval_batch_size\": 128,\n",
      "  \"eval_steps\": 10,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"iterations\": 1000,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"line_count\": 1,\n",
      "  \"lr\": 0.00025,\n",
      "  \"max_steps\": 3600000,\n",
      "  \"model\": \"GPT2\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 256,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 256,\n",
      "  \"n_vocab\": 25000,\n",
      "  \"opt_name\": \"adam\",\n",
      "  \"precision\": \"float32\",\n",
      "  \"predict_batch_size\": 8,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"res_dropout\": 0.1,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"scale_by_depth\": true,\n",
      "  \"scale_by_in\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
      "  \"train_batch_size\": 320,\n",
      "  \"train_steps\": 10000,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25000,\n",
      "  \"warmup_steps\": 2000,\n",
      "  \"weight_decay\": 0.01\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/pytorch_model.bin from cache at /home/sahel/.cache/huggingface/transformers/c15cc7211209da41b45823a6402d55435aca4a83aaf7add6a53a2b59e58a67a2.456cf2e006b146f734c3c33edfcf7d564ae3157b9e8be51da7158c2b8cd05463\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at bolbolzaban/gpt2-persian.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /home/sahel/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"beta1\": 0.9,\n",
      "  \"beta2\": 0.98,\n",
      "  \"bos_token_id\": 8,\n",
      "  \"data_path\": \"\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"embed_dropout\": 0.1,\n",
      "  \"eos_token_id\": 9,\n",
      "  \"epsilon\": 1e-09,\n",
      "  \"eval_batch_size\": 128,\n",
      "  \"eval_steps\": 10,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"iterations\": 1000,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"line_count\": 1,\n",
      "  \"lr\": 0.00025,\n",
      "  \"max_steps\": 3600000,\n",
      "  \"model\": \"GPT2\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 256,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 256,\n",
      "  \"n_vocab\": 25000,\n",
      "  \"opt_name\": \"adam\",\n",
      "  \"precision\": \"float32\",\n",
      "  \"predict_batch_size\": 8,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"res_dropout\": 0.1,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"scale_by_depth\": true,\n",
      "  \"scale_by_in\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
      "  \"train_batch_size\": 320,\n",
      "  \"train_steps\": 10000,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25000,\n",
      "  \"warmup_steps\": 2000,\n",
      "  \"weight_decay\": 0.01\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/spiece.model from cache at /home/sahel/.cache/huggingface/transformers/5150b65e3adedfb2ee4dbf8cf3f2678d10fa7eb47ea60a4e45d91ad65b2ad9ac.cba88ff49c3393ce942262df421f241abe04b1faf9dfa51b28ff9c74dbd0894e\n",
      "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer.json from cache at /home/sahel/.cache/huggingface/transformers/5093edd49280bca5e77d3a2b65210aed0624d625f8e662ba4b7bae9ac368a26a.a4f7a3e25dc07fe97c6be742bc492f8d771c52b091b133266088e9e55a40343f\n",
      "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/special_tokens_map.json from cache at /home/sahel/.cache/huggingface/transformers/f2610d634888d7f6e69ab99f04e9013fbe291b9dfb3e3c0907e1c36215faf094.d4d3bb8ba869519766ba55c28fb7ac26fa04d3dd52b286eb8c77adde589387ed\n",
      "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer_config.json from cache at /home/sahel/.cache/huggingface/transformers/917016d11d789feaa0782d8c93b7e65adfb036276c3c8f44bb354733033ee3ef.5d97eae2282178dfbf0a4d70367a49303af74d87d1b86eb8da118577fa61246f\n",
      "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /home/sahel/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"beta1\": 0.9,\n",
      "  \"beta2\": 0.98,\n",
      "  \"bos_token_id\": 8,\n",
      "  \"data_path\": \"\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"embed_dropout\": 0.1,\n",
      "  \"eos_token_id\": 9,\n",
      "  \"epsilon\": 1e-09,\n",
      "  \"eval_batch_size\": 128,\n",
      "  \"eval_steps\": 10,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"iterations\": 1000,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"line_count\": 1,\n",
      "  \"lr\": 0.00025,\n",
      "  \"max_steps\": 3600000,\n",
      "  \"model\": \"GPT2\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 256,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 256,\n",
      "  \"n_vocab\": 25000,\n",
      "  \"opt_name\": \"adam\",\n",
      "  \"precision\": \"float32\",\n",
      "  \"predict_batch_size\": 8,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"res_dropout\": 0.1,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"scale_by_depth\": true,\n",
      "  \"scale_by_in\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
      "  \"train_batch_size\": 320,\n",
      "  \"train_steps\": 10000,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25000,\n",
      "  \"warmup_steps\": 2000,\n",
      "  \"weight_decay\": 0.01\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sahel/university/NLP/.venv/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file ./data/cached_lm_AlbertTokenizerFast_254_ferdousi.txt [took 0.015 s]\n",
      "Loading features from cached file ./data/cached_lm_AlbertTokenizerFast_254_test.txt [took 0.000 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelWithLMHead,\n",
    ")\n",
    "\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=train_path, block_size=256\n",
    "    )\n",
    "\n",
    "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "\n",
    "# Freezing the lower layers increases the training speed and reduces the memory requirement.\n",
    "# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n",
    "# or unfreeze as many layers that you can affort training with a reasonable batchsize.\n",
    "def freeze_lower_layers():\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in (\n",
    "        model.base_model.h[23].parameters() or model.base_model.h[22].parameters()\n",
    "    ):\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "# load model\n",
    "model = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
    "\n",
    "# freeze lower layers and only train top layers\n",
    "freeze_lower_layers()\n",
    "\n",
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
    "train_dataset, test_dataset, data_collator = load_dataset(\n",
    "    \"./data/ferdousi.txt\", \"./data/test.txt\", tokenizer\n",
    ")\n",
    "print(type(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64d906e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts = train_test_split(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "156a8c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03004f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(val_texts))\n",
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "936486e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895\n",
      "632\n",
      "2527\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(val_texts))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff25dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1895\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 790\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                    \n",
      "  0%|          | 38/66865 [1:27:32<5:46:23,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2555, 'learning_rate': 5e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A                                                  \n",
      "\n",
      "                                                      \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [2:16:06<00:00, 10.34s/it]/s]\n",
      "Saving model checkpoint to ./model\n",
      "Configuration saved in ./model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8166.6446, 'train_samples_per_second': 1.16, 'train_steps_per_second': 0.097, 'train_loss': 5.17218388424644, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    # Set the batch size to a maximum value that could fit into GPU memory,\n",
    "    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_texts,\n",
    "    eval_dataset=val_texts,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# save\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
