{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xI1r8CqCu7Fh",
    "outputId": "850ea181-141c-4d1b-ca57-ea5d37b1232e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -q wandb\n",
    "! git clone \"https://github.com/amnghd/Persian_poems_corpus.git\"\n",
    "! mkdir \"corpus\"\n",
    "! cp \"Persian_poems_corpus/normalized/ferdousi_norm.txt\" \"Persian_poems_corpus/normalized/hafez_norm.txt\" \"Persian_poems_corpus/normalized/moulavi_norm.txt\"\n",
    "\"./corpus/\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GY8_SYkxrHx1",
    "outputId": "2e38c4cb-968b-4436-8a22-bfa1f805e2c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'Persian_poems_corpus' already exists and is not an empty directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soroush/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf4OM-5VnNsA",
    "outputId": "2654bcd1-3f28-4efa-c236-3e718cf7ea2e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQgc5YEUq1T7",
    "outputId": "442e8cac-e0d6-4740-eee5-9409ac2d2856"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass\n",
    "\n",
    "\n",
    "config = Config()\n",
    "# wandb.init(project=\"ferdousi-generator\", name='all_poem_generic_tune')\n",
    "# config = wandb.config\n",
    "config.max_epochs = 50\n",
    "config.batch_size = 256\n",
    "config.embedding_size = 512\n",
    "config.lstm_num_layers = 3\n",
    "config.lstm_hidden_size = 512\n",
    "config.sequence_length = 10\n",
    "config.log_interval = 10\n",
    "config.learning_rate = 0.001\n",
    "config.vocab_size = 40000\n",
    "config.lstm_dropout = 0.2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hhC7SjF5nNsE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "q9sBywu_nNsF"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset, config, device=torch.device('cpu')):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = config.embedding_size\n",
    "        self.lstm_hidden_size = config.lstm_hidden_size\n",
    "        self.lstm_dropout = 0.2\n",
    "        self.embedding_dim = config.embedding_size\n",
    "        self.num_layers = config.lstm_num_layers\n",
    "        self.device = device\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.lstm_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, self.vocab_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            device=torch.device('cpu'),\n",
    "            poet='ferdousi',\n",
    "            corpus_dir='./Persian_poems_corpus/normalized',\n",
    "            vocab_path='./vocabulary.txt'\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        self.words_by_poet = self.load_words(corpus_dir)\n",
    "        self.poet = poet\n",
    "\n",
    "    def preprocess_lines(self, lines, mask_key):\n",
    "        lines = map(\n",
    "            lambda i, line:\n",
    "            f'[BOM_{mask_key}] ' + line + f' [EOS_{mask_key}]' if i % 2 == 0\n",
    "            else f'[BOM_{mask_key}] ' + line,\n",
    "            enumerate(lines)\n",
    "        )\n",
    "        lines = map(lambda line: line.split(' '), lines)\n",
    "        words = itertools.chain.from_iterable(lines)\n",
    "        return words\n",
    "\n",
    "    def load_words(self, corpus_dir):\n",
    "        words_by_poet = {}\n",
    "        for filename in os.listdir(corpus_dir):\n",
    "            with open(os.path.join(corpus_dir, filename)) as f:\n",
    "                poet_name = filename.split('_')[0]\n",
    "                lines = f.readlines()\n",
    "                words_by_poet[poet_name] = self.preprocess_lines(lines, poet_name)\n",
    "        return words_by_poet\n",
    "\n",
    "    def load_vocabulary(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def all_poets(self):\n",
    "        return self.words_by_poet.keys()\n",
    "\n",
    "    @property\n",
    "    def poet(self):\n",
    "        return self._poet\n",
    "\n",
    "    @poet.setter\n",
    "    def poet(self, poet):\n",
    "        self._poet = poet\n",
    "        if poet == 'all':\n",
    "            self.words = itertools.chain.from_iterable(self.words_by_poet.values())\n",
    "        else:\n",
    "            self.words = self.words_by_poet[poet]\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocabulary())}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocabulary())}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def vocabulary(self):\n",
    "        all_words = itertools.chain.from_iterable(self.words_by_poet.values())\n",
    "        word_counts = Counter(all_words)\n",
    "        vocabulary = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        return vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.config.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensors = (\n",
    "            torch.tensor(self.words_indexes[index:index + self.args.sequence_length]).to(self.device),\n",
    "            torch.tensor(self.words_indexes[index + 1:index + self.args.sequence_length + 1]).to(self.device),\n",
    "        )\n",
    "        return tensors"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KZ_aYCW6nNsG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 1 required positional argument: 'line'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_283329/2396529933.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPoemDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cpu'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpoet\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'all'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'../data/poems'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_283329/397887110.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, device, poet, corpus_dir, vocab_path)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords_by_poet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_words\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpoet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpoet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpreprocess_lines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_283329/397887110.py\u001B[0m in \u001B[0;36mpoet\u001B[0;34m(self, poet)\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords_by_poet\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpoet\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex_to_word\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword_to_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_283329/397887110.py\u001B[0m in \u001B[0;36mvocabulary\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0mall_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mitertools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords_by_poet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0mword_counts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m         \u001B[0mvocabulary\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword_counts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mword_counts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreverse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/collections/__init__.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    566\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'expected at most 1 arguments, got %d'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    567\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCounter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 568\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    569\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    570\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__missing__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/collections/__init__.py\u001B[0m in \u001B[0;36mupdate\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    653\u001B[0m                     \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCounter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterable\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# fast path when counter is empty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 655\u001B[0;31m                 \u001B[0m_count_elements\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    656\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    657\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: <lambda>() missing 1 required positional argument: 'line'"
     ]
    }
   ],
   "source": [
    "dataset = PoemDataset(config, device=torch.device('cpu'), poet='all', corpus_dir='../data/poems')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def train(dataset, model, args, checkpoint_path='/content/drive/MyDrive/NLP Class/checkpoints'):\n",
    "    wandb.watch(model)\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    print({'batch_count': len(dataloader), 'epoch_count': args.max_epochs})\n",
    "    for epoch in range(args.max_epochs):\n",
    "        state_h, state_c = model.init_state(args.sequence_length)\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'epoch': epoch, 'batch': batch, 'loss': loss.item()})\n",
    "            if batch % args.log_interval == 0:\n",
    "                wandb.log({\"loss\": loss})\n",
    "        try:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, os.path.join(checkpoint_path, f'model_{args.name}_checkpoint_{time.time()}.pt'))\n",
    "        except:\n",
    "            pass"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nUkrBNA5nNsH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P5VE6ck8nNsH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([  0,   3,  81, 363, 118,   2,  98,   1, 365, 698]), tensor([  3,  81, 363, 118,   2,  98,   1, 365, 698, 221]))\n",
      "(tensor([  3,  81, 363, 118,   2,  98,   1, 365, 698, 221]), tensor([  81,  363,  118,    2,   98,    1,  365,  698,  221, 3552]))\n",
      "(tensor([  81,  363,  118,    2,   98,    1,  365,  698,  221, 3552]), tensor([ 363,  118,    2,   98,    1,  365,  698,  221, 3552,    0]))\n",
      "(tensor([ 363,  118,    2,   98,    1,  365,  698,  221, 3552,    0]), tensor([ 118,    2,   98,    1,  365,  698,  221, 3552,    0,  363]))\n",
      "(tensor([ 118,    2,   98,    1,  365,  698,  221, 3552,    0,  363]), tensor([   2,   98,    1,  365,  698,  221, 3552,    0,  363,   81]))\n",
      "(tensor([   2,   98,    1,  365,  698,  221, 3552,    0,  363,   81]), tensor([  98,    1,  365,  698,  221, 3552,    0,  363,   81,    2]))\n",
      "(tensor([  98,    1,  365,  698,  221, 3552,    0,  363,   81,    2]), tensor([   1,  365,  698,  221, 3552,    0,  363,   81,    2,  363]))\n",
      "(tensor([   1,  365,  698,  221, 3552,    0,  363,   81,    2,  363]), tensor([ 365,  698,  221, 3552,    0,  363,   81,    2,  363,   59]))\n",
      "(tensor([ 365,  698,  221, 3552,    0,  363,   81,    2,  363,   59]), tensor([ 698,  221, 3552,    0,  363,   81,    2,  363,   59,    1]))\n",
      "(tensor([ 698,  221, 3552,    0,  363,   81,    2,  363,   59,    1]), tensor([ 221, 3552,    0,  363,   81,    2,  363,   59,    1,  363]))\n"
     ]
    }
   ],
   "source": [
    "dataset = PoemDataset(config, device)\n",
    "# get first 10 items in dataset\n",
    "for i in range(10):\n",
    "    print(dataset[i])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0nkVsXnnNsI",
    "outputId": "114da38b-d01f-4968-8d8c-01d6f6e1fbaa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17763"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(dataset.uniq_words)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlBfFZC8nNsI",
    "outputId": "4801ef59-f393-4cb8-a212-84a5d032ba79"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Model(dataset, config, device)\n",
    "\n",
    "train(dataset, model, config)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aNDW-n0TnNsJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(dataset, config, device)\n",
    "chechkpoint = torch.load('/content/drive/MyDrive/NLP Class/checkpoints/model_checkpoint_1654448961.97679.pt',\n",
    "                         map_location=torch.device('cpu'))\n",
    "print(chechkpoint['epoch'])\n",
    "model.load_state_dict(chechkpoint['model_state_dict'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vQI80qiTLb8",
    "outputId": "309bffc5-18d2-4b22-ed24-5810d6421102"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[BOM]\n",
      "توانا\n",
      "بود\n",
      "هر\n",
      "که\n",
      "نامه\n",
      "سرآرد\n",
      "خرد\n",
      "بر\n",
      "سرش\n",
      "[BOM]\n",
      "بگویید\n",
      "کایند\n",
      "پوشیده\n",
      "نو\n",
      "[BOS]\n",
      "ستودم\n",
      "به\n",
      "پیش\n",
      "سپاه\n",
      "بهشت\n",
      "[BOM]\n",
      "نگوییم\n",
      "گفتند\n",
      "بیدار\n",
      "هیچ\n",
      "[BOS]\n",
      "ز\n",
      "فرزند\n",
      "بیدار\n",
      "بر\n",
      "لاژورد\n",
      "[BOM]\n",
      "و\n",
      "فرخ\n",
      "آسیا\n",
      "چون\n",
      "نباشد\n",
      "بگاه\n",
      "[BOS]\n",
      "خداوند\n",
      "خاک\n",
      "اختر\n",
      "و\n",
      "جنگ\n",
      "دار\n",
      "[BOM]\n",
      "که\n",
      "باشند\n",
      "او\n",
      "را\n",
      "به\n",
      "تن\n",
      "را\n",
      "سپرد\n",
      "[BOS]\n",
      "کسی\n",
      "را\n",
      "فرستاده\n",
      "را\n",
      "بافتن\n",
      "[BOM]\n",
      "برین\n",
      "گونه\n",
      "آمد\n",
      "ببالید\n",
      "و\n",
      "گوی\n",
      "[BOS]\n",
      "سپرهای\n",
      "شاهان\n",
      "زرینه\n",
      "کفش\n",
      "[BOM]\n",
      "سوی\n",
      "خیمه\n",
      "او\n",
      "بدخواه\n",
      "شد\n",
      "[BOS]\n",
      "همی\n",
      "خوار\n",
      "گویی\n",
      "همی\n",
      "بود\n",
      "شاه\n",
      "[BOM]\n",
      "همه\n",
      "ساختن\n",
      "و\n",
      "بنه\n",
      "برنهاد\n",
      "[BOS]\n",
      "به\n",
      "سر\n",
      "برنهاد\n",
      "و\n",
      "کرانه\n",
      "دلیر\n",
      "[BOM]\n",
      "بخرید\n",
      "فرزند\n",
      "هنگام\n",
      "ننگ\n",
      "[BOS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n'.join(predict(dataset, model, text='[BOM] توانا بود هر که')))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rAGcmLZ2nNsJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e1ce9911-7781-4703-af4b-56e7894f9bc0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aAfSTSsfnNsK",
    "outputId": "b5d4b70e-3972-461e-b573-4cf66a52a426"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save torch model and configs\n",
    "import time\n",
    "\n",
    "torch.save({'model_state_dict': model.state_dict()}, f'../data/checkpoints/model_{time.time()}.pt')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4nAW3owVnNsK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27a225e506849d1bdf80008d630248f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▆▅▅▆▅▃▃▄▃▃▃▃▂▃▃▃▃▃▃▃▃▂▁▃▃▂▂▂▂▂▂▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>4.09747</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">dark-butterfly-5</strong>: <a href=\"https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o\" target=\"_blank\">https://wandb.ai/soroushtabesh/ferdousi-generator/runs/38pzbi2o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220604_214452-38pzbi2o/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "referenced_widgets": [
      "a27a225e506849d1bdf80008d630248f"
     ]
    },
    "id": "k3OKozxCnNsK",
    "outputId": "0e015429-bfa2-4342-91ed-e652878e5b71"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "NLP-HW3-LSTM.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}