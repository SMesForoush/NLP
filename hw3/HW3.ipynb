{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b2faf6d",
      "metadata": {
        "id": "8b2faf6d"
      },
      "source": [
        "\n",
        "</h1> <h1 style='direction:rtl; font-family: \"B Lotus\";'> ÿ™ŸÖÿ±€åŸÜ ÿØŸàŸÖ - Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ¥ÿπÿ±Ÿáÿß€å ÿ®ÿß ŸÑŸáÿ¨Ÿá‚Äå€å ÿ¥€åÿ±ÿßÿ≤€å\n",
        "\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "947d3ef6",
      "metadata": {
        "id": "947d3ef6"
      },
      "source": [
        "<h2 style='direction:rtl;font-family: \"B Lotus\";'> ÿØÿßÿØŸá‚Äå€å Ÿàÿ±ŸàÿØ€å </h2> \n",
        "\n",
        "<p style='direction:rtl;font-family: \"B Lotus\";' >\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3251a5c1",
      "metadata": {
        "id": "3251a5c1"
      },
      "outputs": [],
      "source": [
        "path_dir = \".\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4fe8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4fe8e2",
        "outputId": "de1a16d9-f71f-4d5e-a483-65c9b2c5bc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 316 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.4 MB 57.2 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=ddbbe1b0064828728764ad7741952c30584435c9958055d9ed2582cd1cbb3e1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154526 sha256=e3a285cc6b37445a4a8d7d7a5a25d9f9736be16436ba8f8677f46e1c4cb94063\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.2 MB 8.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas\n",
        "! pip install hazm\n",
        "! pip install transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab86f6b",
      "metadata": {
        "id": "9ab86f6b"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8425cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8425cfb",
        "outputId": "1d76e685-8a26-4380-947e-7d7f6315b89c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1902c48a",
      "metadata": {
        "id": "1902c48a"
      },
      "outputs": [],
      "source": [
        "class Cleaner:\n",
        "    def __init__(self):\n",
        "        self.normalizer = Normalizer()\n",
        "        # self.stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('data/stopwords.txt','r','utf-8').readlines()]\n",
        "        self.file_range = None\n",
        "\n",
        "\n",
        "    def read_poems(self, file_patterns: str, file_range: tuple, normalize=False):  \n",
        "        file_range = range(*file_range) if file_range is not None else None \n",
        "        file_names = glob.glob(file_patterns)\n",
        "        mesra_collection = []\n",
        "        for file_name in file_names:\n",
        "            if normalize:\n",
        "                mesra_collection += [self.normalizer.normalize(x) for x in codecs.open(file_name,'rU','utf-8').readlines()]\n",
        "            else:\n",
        "                mesra_collection += codecs.open(file_name,'rU','utf-8').readlines()[2:]\n",
        "        return mesra_collection\n",
        "    \n",
        "\n",
        "    def read_documents(self):\n",
        "        poems = self.read_poems(self.file_pattern, self.file_range)\n",
        "        cleaned = self.clean_data(poems)\n",
        "        return ' \\n '.join(cleaned)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b0bd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b0bd15",
        "outputId": "0116c322-0947-4283-8522-2682d531889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99217\n"
          ]
        }
      ],
      "source": [
        "path=f\"{path_dir}/ferdousi.txt\"\n",
        "cleaner = Cleaner()\n",
        "mesras = cleaner.read_poems(file_patterns=path, file_range=None, normalize=False)\n",
        "print(len(mesras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5e8521",
      "metadata": {
        "id": "dd5e8521",
        "outputId": "3b0a2062-3405-4cb2-e56f-f513e1d75b9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 452M/452M [03:31<00:00, 2.23MB/s]   \n",
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['ŸÖÿß',\n",
              " 'ÿØÿ±',\n",
              " 'ŸáŸàÿ¥',\n",
              " '[ZWNJ]',\n",
              " 'Ÿàÿßÿ±Ÿá',\n",
              " 'ŸÖÿπÿ™ŸÇÿØ€åŸÖ',\n",
              " 'ÿ®ÿß',\n",
              " 'ÿßŸÜÿ™ŸÇÿßŸÑ',\n",
              " 'ÿµÿ≠€åÿ≠',\n",
              " 'ÿØÿßŸÜÿ¥',\n",
              " 'Ÿà',\n",
              " 'ÿ¢',\n",
              " '##⁄ØÿßŸá€å',\n",
              " 'ÿå',\n",
              " 'ŸáŸÖŸá',\n",
              " 'ÿßŸÅÿ±ÿßÿØ',\n",
              " 'ŸÖ€åÿ™ŸàÿßŸÜŸÜÿØ',\n",
              " 'ÿßÿ≤',\n",
              " 'ÿßÿ®ÿ≤ÿßÿ±Ÿáÿß€å',\n",
              " 'ŸáŸàÿ¥ŸÖŸÜÿØ',\n",
              " 'ÿßÿ≥ÿ™ŸÅÿßÿØŸá',\n",
              " '⁄©ŸÜŸÜÿØ',\n",
              " '.',\n",
              " 'ÿ¥ÿπÿßÿ±',\n",
              " 'ŸÖÿß',\n",
              " 'ŸáŸàÿ¥',\n",
              " 'ŸÖÿµŸÜŸàÿπ€å',\n",
              " 'ÿ®ÿ±ÿß€å',\n",
              " 'ŸáŸÖŸá',\n",
              " 'ÿßÿ≥ÿ™',\n",
              " '.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
        "\n",
        "# v3.0\n",
        "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
        "model = AutoModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "text = \"ŸÖÿß ÿØÿ± ŸáŸàÿ¥‚ÄåŸàÿßÿ±Ÿá ŸÖÿπÿ™ŸÇÿØ€åŸÖ ÿ®ÿß ÿßŸÜÿ™ŸÇÿßŸÑ ÿµÿ≠€åÿ≠ ÿØÿßŸÜÿ¥ Ÿà ÿ¢⁄ØÿßŸá€åÿå ŸáŸÖŸá ÿßŸÅÿ±ÿßÿØ ŸÖ€åÿ™ŸàÿßŸÜŸÜÿØ ÿßÿ≤ ÿßÿ®ÿ≤ÿßÿ±Ÿáÿß€å ŸáŸàÿ¥ŸÖŸÜÿØ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜŸÜÿØ. ÿ¥ÿπÿßÿ± ŸÖÿß ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿ®ÿ±ÿß€å ŸáŸÖŸá ÿßÿ≥ÿ™.\"\n",
        "tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7dd04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7dd04d",
        "outputId": "da16e9d2-bad0-4973-8ac7-f7117de1fbb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    BertModel\n",
        ")\n",
        "\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer, file_path=train_path, block_size=256\n",
        "    )\n",
        "\n",
        "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "\n",
        "# Freezing the lower layers increases the training speed and reduces the memory requirement.\n",
        "# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n",
        "# or unfreeze as many layers that you can affort training with a reasonable batchsize.\n",
        "def freeze_lower_layers():\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in (\n",
        "        model.base_model.h[23].parameters() or model.base_model.h[22].parameters()\n",
        "    ):\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "# load model\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "\n",
        "# freeze lower layers and only train top layers\n",
        "freeze_lower_layers()\n",
        "\n",
        "# load dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "train_dataset, test_dataset, data_collator = load_dataset(\n",
        "    \"ferdousi.txt\", \"test.txt\", tokenizer\n",
        ")\n",
        "print(type(train_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gXpjLVYYaXjQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpjLVYYaXjQ",
        "outputId": "bdc3fedc-9e85-40c5-af47-5ebc13776b90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/5150b65e3adedfb2ee4dbf8cf3f2678d10fa7eb47ea60a4e45d91ad65b2ad9ac.cba88ff49c3393ce942262df421f241abe04b1faf9dfa51b28ff9c74dbd0894e\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5093edd49280bca5e77d3a2b65210aed0624d625f8e662ba4b7bae9ac368a26a.a4f7a3e25dc07fe97c6be742bc492f8d771c52b091b133266088e9e55a40343f\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/f2610d634888d7f6e69ab99f04e9013fbe291b9dfb3e3c0907e1c36215faf094.d4d3bb8ba869519766ba55c28fb7ac26fa04d3dd52b286eb8c77adde589387ed\n",
            "loading file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/917016d11d789feaa0782d8c93b7e65adfb036276c3c8f44bb354733033ee3ef.5d97eae2282178dfbf0a4d70367a49303af74d87d1b86eb8da118577fa61246f\n",
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"bolbolzaban/gpt2-persian\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/55af8b32edc9c441878906a22de9b0c79dce98654714626407f7ea3ee7fc7349.1f2d676354b04ed818190819fee0d7d2deb97cfe456b804edfea24e6a8053fd5\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"./\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout\": 0.1,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.98,\n",
            "  \"bos_token_id\": 8,\n",
            "  \"data_path\": \"\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"embed_dropout\": 0.1,\n",
            "  \"eos_token_id\": 9,\n",
            "  \"epsilon\": 1e-09,\n",
            "  \"eval_batch_size\": 128,\n",
            "  \"eval_steps\": 10,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"iterations\": 1000,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"line_count\": 1,\n",
            "  \"lr\": 0.00025,\n",
            "  \"max_steps\": 3600000,\n",
            "  \"model\": \"GPT2\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 256,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 256,\n",
            "  \"n_vocab\": 25000,\n",
            "  \"opt_name\": \"adam\",\n",
            "  \"precision\": \"float32\",\n",
            "  \"predict_batch_size\": 8,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"res_dropout\": 0.1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"scale_by_depth\": true,\n",
            "  \"scale_by_in\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"AlbertTokenizer\",\n",
            "  \"train_batch_size\": 320,\n",
            "  \"train_steps\": 10000,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 25000,\n",
            "  \"warmup_steps\": 2000,\n",
            "  \"weight_decay\": 0.01\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c15cc7211209da41b45823a6402d55435aca4a83aaf7add6a53a2b59e58a67a2.456cf2e006b146f734c3c33edfcf7d564ae3157b9e8be51da7158c2b8cd05463\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at bolbolzaban/gpt2-persian.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "freeze_lower_layers()\n",
        "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
        "sample = generator('ÿØÿ± €å⁄© ÿßÿ™ŸÅÿßŸÇ ÿ¥⁄ØŸÅÿ™ ÿßŸÜ⁄Ø€åÿ≤ÿå Ÿæ⁄òŸàŸáÿ¥⁄Øÿ±ÿßŸÜ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OyvnTBDCdH3s",
      "metadata": {
        "id": "OyvnTBDCdH3s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aLCtmbDhbkPB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCtmbDhbkPB",
        "outputId": "c1948abf-ad6d-4c41-9509-b7f7590e7016"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': ' ÿ™ŸàÿßŸÜÿß ÿ®ŸàÿØ Ÿáÿ±⁄©Ÿá ÿßŸà ÿ±ÿß €åÿßÿ±€å ⁄©ÿ±ÿØ.'}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator(' ÿ™ŸàÿßŸÜÿß ÿ®ŸàÿØ Ÿáÿ±⁄©Ÿá')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9sdNgE4abLmi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sdNgE4abLmi",
        "outputId": "c9e604cb-44e7-414e-bed5-56a732f1635c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'ÿØÿ± €å⁄© ÿßÿ™ŸÅÿßŸÇ ÿ¥⁄ØŸÅÿ™ ÿßŸÜ⁄Ø€åÿ≤ÿå Ÿæ⁄òŸàŸáÿ¥⁄Øÿ±ÿßŸÜ ÿ¢ŸÖÿ±€å⁄©ÿß€å€å ÿå ŸÖŸàŸÅŸÇ ÿ®Ÿá ⁄©ÿ¥ŸÅ ÿ¢ŸÜ ÿ¥ÿØŸÜÿØ.'}]\n"
          ]
        }
      ],
      "source": [
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d906e9",
      "metadata": {
        "id": "64d906e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts = train_test_split(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156a8c43",
      "metadata": {
        "id": "156a8c43",
        "outputId": "17e5440f-9532-4edc-fb1f-3c69bb594aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03004f61",
      "metadata": {
        "id": "03004f61",
        "outputId": "5809efaf-8475-4ba9-f79e-63aae641d416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'transformers.data.datasets.language_modeling.TextDataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(val_texts))\n",
        "print(type(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936486e8",
      "metadata": {
        "id": "936486e8",
        "outputId": "149b9b9c-b3ef-449b-9d04-bc0112636736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1895\n",
            "632\n",
            "2527\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff25dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "9ff25dd9",
        "outputId": "28249912-10a7-41a0-9f22-c3b43be8e8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-17eb82a5a28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 14.76 GiB total capacity; 13.22 GiB already allocated; 93.75 MiB free; 13.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# train\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    # Set the batch size to a maximum value that could fit into GPU memory,\n",
        "    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_texts,\n",
        "    eval_dataset=val_texts,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NV73xepfgy90",
      "metadata": {
        "id": "NV73xepfgy90"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2857862f",
      "metadata": {},
      "source": [
        "<style>\n",
        "@font-face {font-family: \"B Nazanin\"; src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot\"); src: url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/3671adca6f650c92b83f906e49656986.svg#B Nazanin\") format(\"svg\"); }\n",
        "    </style>\n",
        "<div dir=\"rtl\" align=\"center\" style ='font-family: \"B Nazanin\";'>\n",
        "    <h1>\n",
        "        ÿ™ŸÖÿ±€åŸÜ ÿ≥ŸàŸÖ ÿØÿ±ÿ≥ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ‚ÄåŸáÿß€å ÿ∑ÿ®€åÿπ€å\n",
        "    </h1>\n",
        "    <h3>\n",
        "        ⁄Øÿ±ÿØÿ¢Ÿàÿ±ŸÜÿØ⁄ØÿßŸÜ:<br/>\n",
        "        ÿ≥ÿßÿ≠ŸÑ ŸÖÿ≥‚ÄåŸÅÿ±Ÿàÿ¥ÿå ÿ≥ÿ±Ÿàÿ¥ ÿ™ÿßÿ®ÿ¥ÿå ÿØÿ±ŸÜÿß ÿØŸáŸÇÿßŸÜ€å\n",
        "    </h3>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    ŸÖÿß ÿØÿ± ÿß€åŸÜ ÿ™ŸÖÿ±€åŸÜ ÿ™ÿ±⁄© \"ÿ™⁄©ŸÖ€åŸÑ ŸÖÿµÿ±ÿßÿπ ÿØŸàŸÖ ÿ®ÿß ÿ±ÿπÿß€åÿ™ Ÿàÿ≤ŸÜ ÿ¥ÿπÿ±\" ÿ±ÿß ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ÿ±ÿØ€åŸÖ. ÿØÿ± ÿß€åŸÜ ÿ™ŸÖÿ±€åŸÜ ŸÇÿµÿØ ÿØÿßÿ±€åŸÖ ÿ®Ÿá ⁄©ŸÖ⁄© ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ≤ÿ®ÿßŸÜ€å ŸÖŸÜÿßÿ≥ÿ®ÿå ⁄ÜŸÜÿØ ⁄©ŸÑŸÖŸá ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÖÿµÿ±ÿπ ÿßÿ≤ ÿ®€åÿ™ ÿ±ÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿàÿ±ŸàÿØ€å ÿ®⁄Ø€åÿ±€åŸÖ Ÿà ⁄ÜŸÜÿØ ⁄©ŸÑŸÖŸá ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿµÿ±ÿπ ÿØŸàŸÖ ÿß€åŸÜ ÿ®€åÿ™ ÿ±ÿß ÿÆÿ±Ÿàÿ¨€å ÿØŸá€åŸÖ. ÿØÿ± ÿß€åŸÜ ÿ™ŸÖÿ±€åŸÜ ŸÖÿß ÿßÿ≤ Ÿàÿ®ÿ≥ÿß€åÿ™ ⁄ØŸÜÿ¨Ÿàÿ± ÿ®ÿ±ÿß€å ÿ¨ŸÖÿπ‚Äåÿ¢Ÿàÿ±€å ÿØÿßÿØŸá ⁄©ŸÖ⁄© ⁄Øÿ±ŸÅÿ™€åŸÖ Ÿà ÿßÿ≤ ⁄ÜŸÜÿØ€åŸÜ ŸÇÿßŸÑÿ® ÿ¥ÿπÿ±€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ÿ±ÿØ€åŸÖÿå ÿ≤€åÿ±ÿß ŸÇÿßŸÅ€åŸá ÿØÿßÿ¥ÿ™ŸÜ €åÿß ŸÜÿØÿßÿ¥ÿ™ŸÜ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿàÿ±ŸàÿØ€å ÿØÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ. ÿß⁄Øÿ± ŸÜ€åÿßÿ≤ ÿ®Ÿá ŸáŸÖ‚ÄåŸÇÿßŸÅ€åŸá ÿ®ŸàÿØŸÜ ÿØŸà ŸÖÿµÿ±ÿßÿπ ÿ®ŸàÿØÿå ÿßÿ≤ ŸÇÿßŸÑÿ® ŸÖÿ´ŸÜŸà€å ŸÜÿ∏€åÿ± ÿ¥ÿßŸáŸÜŸÖÿßŸá‚Äå€å ŸÅÿ±ÿØŸàÿ≥€å ⁄©ŸÖ⁄© ŸÖ€å‚Äå⁄Ø€åÿ±€åŸÖÿõ Ÿà ÿß⁄Øÿ± ŸÜ€åÿßÿ≤ ÿ®Ÿá ŸáŸÖ‚ÄåŸÇÿßŸÅ€åŸá ÿ®ŸàÿØŸÜ ÿØŸà ŸÖÿµÿ±ÿßÿπ ŸÜÿ®ŸàÿØ ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åŸÖ ÿßÿ≤ ŸÇÿßŸÑÿ® ŸÇÿ∑ÿπŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åŸÖ. <br>\n",
        "    ÿ®ÿ±ÿß€å ÿ≥ÿßÿÆÿ™ ⁄ÜŸÜ€åŸÜ ÿ≥€åÿ≥ÿ™ŸÖ€å ÿßÿ≤ ⁄ÜŸÜÿØ ŸÖÿØŸÑ ÿ≤ÿ®ÿßŸÜ€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ: ŸÖÿØŸÑ n-gramÿå ŸÖÿØŸÑ encoder-decoder ÿ®ÿß €å⁄© ÿ¥ÿ®⁄©Ÿá LSTM ÿ®Ÿá ÿπŸÜŸàÿßŸÜ encoderÿå Ÿà ŸÖÿØŸÑ€å Ÿæ€åÿ±Ÿà ŸÖ⁄©ÿßŸÜ€åÿ≥ŸÖ ÿ™Ÿàÿ¨Ÿá.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0571e",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\"  style ='font-family: \"B Nazanin\";'>\n",
        "    <h3> 1. ÿØŸàÿ®ÿßÿ±Ÿá‚ÄåŸÜŸà€åÿ≥€å ÿØÿßÿØŸá </h3>\n",
        "    ÿßÿ≤ ÿ¢ŸÜÿ¨ÿß€å€å ⁄©Ÿá ÿØÿ± ÿØÿßÿØŸá‚Äå€å ⁄Øÿ±ŸÅÿ™Ÿá ÿ¥ÿØŸáÿå ÿßÿ®ÿ™ÿØÿß Ÿà ÿßŸÜÿ™Ÿáÿß€å ÿßÿ®€åÿßÿ™ Ÿà ŸÖÿµÿ±ÿßÿπ‚ÄåŸáÿß ŸÖÿ¥ÿÆÿµ ŸÜÿ®ŸàÿØÿå ÿ®Ÿá ⁄©ŸÖ⁄© ⁄©ÿØ ÿ≤€åÿ± ÿß€åŸÜ ÿØÿßÿØŸá ÿ±ÿß ÿ®ÿßÿ≤ŸÜŸà€åÿ≥€å ⁄©ÿ±ÿØ€åŸÖ. ÿßÿ®ÿ™ÿØÿß Ÿà ÿßŸÜÿ™Ÿáÿß€å ÿßÿ®€åÿßÿ™ ÿ®ÿß __BOB__ Ÿà __EOB__ Ÿà ÿßÿ®ÿ™ÿØÿß Ÿà ÿßŸÜÿ™Ÿáÿß€å ŸÖÿµÿ±ÿßÿπ‚ÄåŸáÿß ÿ®ÿß __BOM__ Ÿà __EOM__ ŸÖÿ¥ÿÆÿµ ⁄Øÿ±ÿØ€åÿØ.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b238822b",
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'ferdousi_norm.txt'\n",
        "\n",
        "beyt_file = ''\n",
        "with open(path, 'r', encoding=\"utf-8\") as fp:\n",
        "    lines = fp.readlines()\n",
        "    for i in range(0, len(lines) - 1, 2):\n",
        "        mesra1 = '__BOM__ ' + lines[i].strip() + ' __EOM__ '\n",
        "        mesra2 = '__BOM__ ' + lines[i + 1].strip() + ' __EOM__'\n",
        "        b = '__BOB__ ' + mesra1.strip() + ' ' + mesra2.strip() + ' __EOB__\\n'\n",
        "        beyt_file += b\n",
        "        \n",
        "with open('f_beyt.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "    fp.write(beyt_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b04b67f68733743fa4bd20e2b208bd3fb17523e8f420adf77b7736387b23b0f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
