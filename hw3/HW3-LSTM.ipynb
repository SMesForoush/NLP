{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xI1r8CqCu7Fh",
    "outputId": "850ea181-141c-4d1b-ca57-ea5d37b1232e"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 23:04:19 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| N/A   62C    P0    N/A /  N/A |    710MiB /  2002MiB |     15%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1322      G   /usr/lib/xorg/Xorg                104MiB |\r\n",
      "|    0   N/A  N/A      2644      G   /usr/lib/xorg/Xorg                367MiB |\r\n",
      "|    0   N/A  N/A      2823      G   /usr/bin/gnome-shell               56MiB |\r\n",
      "|    0   N/A  N/A     12966      G   ...AAAAAAAAA= --shared-files       28MiB |\r\n",
      "|    0   N/A  N/A    277211      G   ...AAAAAAAA== --shared-files       26MiB |\r\n",
      "|    0   N/A  N/A    277237      G   ...AAAAAAAAA= --shared-files       18MiB |\r\n",
      "|    0   N/A  N/A    277866      G   ...f_3329.log --shared-files       13MiB |\r\n",
      "|    0   N/A  N/A    329575      G   ...748528155973965074,131072       82MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -q wandb\n",
    "! git clone \"https://github.com/amnghd/Persian_poems_corpus.git\"\n",
    "! mkdir \"corpus\"\n",
    "! cp \"Persian_poems_corpus/normalized/ferdousi_norm.txt\" \"Persian_poems_corpus/normalized/hafez_norm.txt\" \"Persian_poems_corpus/normalized/moulavi_norm.txt\"\n",
    "\"./corpus/\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GY8_SYkxrHx1",
    "outputId": "2e38c4cb-968b-4436-8a22-bfa1f805e2c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'Persian_poems_corpus' already exists and is not an empty directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.is_available())\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf4OM-5VnNsA",
    "outputId": "2654bcd1-3f28-4efa-c236-3e718cf7ea2e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQgc5YEUq1T7",
    "outputId": "442e8cac-e0d6-4740-eee5-9409ac2d2856"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass\n",
    "\n",
    "\n",
    "wandb_active = False\n",
    "project_name = 'poem_generator'\n",
    "run_name = 'all_poem_train'\n",
    "checkpoints_dir = '../data/checkpoints/'\n",
    "corpus_dir = '../data/poems/'\n",
    "vocab_path = '../data/vocabulary.txt'\n",
    "\n",
    "if wandb_active:\n",
    "    wandb.init(project=project_name, name=run_name)\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = Config()\n",
    "config.batch_size = 256\n",
    "config.embedding_size = 512\n",
    "config.lstm_num_layers = 3\n",
    "config.lstm_hidden_size = 512\n",
    "config.sequence_length = 10\n",
    "config.log_interval = 10\n",
    "config.learning_rate = 0.001\n",
    "config.vocab_size = 38590\n",
    "config.lstm_dropout = 0.2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hhC7SjF5nNsE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "id": "q9sBywu_nNsF"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config, device=torch.device('cpu')):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = config.embedding_size\n",
    "        self.lstm_hidden_size = config.lstm_hidden_size\n",
    "        self.lstm_dropout = 0.2\n",
    "        self.embedding_dim = config.embedding_size\n",
    "        self.num_layers = config.lstm_num_layers\n",
    "        self.device = device\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.lstm_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, self.vocab_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "38590"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            device=torch.device('cpu'),\n",
    "            poet='ferdousi',\n",
    "            corpus_dir='./Persian_poems_corpus/normalized',\n",
    "            vocab_path='./vocabulary.txt'\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.vocab_path = vocab_path\n",
    "\n",
    "        self.words_by_poet = self.load_words(corpus_dir)\n",
    "        self.vocabulary = self.load_vocabulary()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.poet = poet\n",
    "\n",
    "    def preprocess_lines(self, lines, mask_key):\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = filter(lambda line: len(line) > 0, lines)\n",
    "        lines = map(lambda line: line.replace('\\n', ''), lines)\n",
    "        lines = map(lambda line: line.replace('\\t', ''), lines)\n",
    "        lines = map(lambda line: line.replace('\\r', ''), lines)\n",
    "        lines = map(\n",
    "            lambda index_line:\n",
    "            f'[BOM_{mask_key}] ' + index_line[1] + ' [EOS]' if index_line[0] % 2 == 1\n",
    "            else f'[BOM_{mask_key}] ' + index_line[1],\n",
    "            enumerate(lines)\n",
    "        )\n",
    "        words = itertools.chain.from_iterable(map(lambda line: line.split(' '), lines))\n",
    "        words = filter(lambda word: len(word) > 0, words)\n",
    "        words = list(words)\n",
    "        return words\n",
    "\n",
    "    def load_words(self, corpus_dir):\n",
    "        words_by_poet = {}\n",
    "        for filename in os.listdir(corpus_dir):\n",
    "            with open(os.path.join(corpus_dir, filename)) as f:\n",
    "                poet_name = filename.split('_')[0]\n",
    "                lines = f.readlines()\n",
    "                words_by_poet[poet_name] = self.preprocess_lines(lines, poet_name)\n",
    "        return words_by_poet\n",
    "\n",
    "    def load_vocabulary(self):\n",
    "        with open(self.vocab_path) as f:\n",
    "            vocabulary = f.readlines()\n",
    "        vocabulary = [word.strip() for word in vocabulary]\n",
    "        return vocabulary\n",
    "\n",
    "    @property\n",
    "    def all_poets(self):\n",
    "        return self.words_by_poet.keys()\n",
    "\n",
    "    @property\n",
    "    def poet(self):\n",
    "        return self._poet\n",
    "\n",
    "    @poet.setter\n",
    "    def poet(self, poet):\n",
    "        self._poet = poet\n",
    "        if poet == 'all':\n",
    "            self.words = list(itertools.chain.from_iterable(self.words_by_poet.values()))\n",
    "        else:\n",
    "            self.words = self.words_by_poet[poet]\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.config.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensors = (\n",
    "            torch.tensor(self.words_indexes[index:index + self.config.sequence_length]).to(self.device),\n",
    "            torch.tensor(self.words_indexes[index + 1:index + self.config.sequence_length + 1]).to(self.device),\n",
    "        )\n",
    "        return tensors\n",
    "\n",
    "\n",
    "dataset = PoemDataset(config, device=torch.device('cpu'), poet='all', corpus_dir=corpus_dir, vocab_path=vocab_path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(dataset[i])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KZ_aYCW6nNsG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this cell to generate new vocabulary\n",
    "\n",
    "all_words = list(itertools.chain.from_iterable(dataset.words_by_poet.values()))\n",
    "word_counts = Counter(all_words)\n",
    "vocab = sorted(list(word_counts))\n",
    "with open(vocab_path, 'w') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def train(dataset, model, config, checkpoint_path='../data/checkpoints', max_epochs=10, ):\n",
    "    if wandb_active:\n",
    "        wandb.watch(model)\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    print({'batch_count': len(dataloader), 'epoch_count': max_epochs})\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(config.sequence_length)\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'epoch': epoch, 'batch': batch, 'loss': loss.item()})\n",
    "            if wandb_active and batch % config.log_interval == 0:\n",
    "                wandb.log({\"loss\": loss})\n",
    "        try:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, os.path.join(checkpoint_path, f'{run_name}_checkpoint_{epoch}_{time.time()}.pt'))\n",
    "        except:\n",
    "            pass"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nUkrBNA5nNsH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, max_predict_length=12):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    i = 0\n",
    "    while words[-1] != '[EOS]' and i < max_predict_length:\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        i += 1\n",
    "    return words"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P5VE6ck8nNsH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## fresh model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model = Model(config, device)\n",
    "dataset = PoemDataset(config, device=device, poet='all', corpus_dir=corpus_dir, vocab_path=vocab_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_count': 4789, 'epoch_count': 10}\n",
      "{'epoch': 0, 'batch': 0, 'loss': 10.556665420532227}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 10.528473854064941}\n",
      "{'epoch': 0, 'batch': 2, 'loss': 10.473825454711914}\n",
      "{'epoch': 0, 'batch': 3, 'loss': 10.087091445922852}\n",
      "{'epoch': 0, 'batch': 4, 'loss': 9.208667755126953}\n",
      "{'epoch': 0, 'batch': 5, 'loss': 8.61754035949707}\n",
      "{'epoch': 0, 'batch': 6, 'loss': 8.090521812438965}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_329939/3026618415.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# train on generic dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheckpoints_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mwandb_active\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mwandb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinish\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_329939/1830750040.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(dataset, model, config, checkpoint_path, max_epochs)\u001B[0m\n\u001B[1;32m     14\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m             \u001B[0my_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mstate_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_c\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mstate_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_c\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_329939/3906434322.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, prev_state)\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0membed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprev_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m         \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1690\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddmm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1691\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1692\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1693\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbias\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1694\u001B[0m             \u001B[0moutput\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train on generic dataset\n",
    "train(dataset, model, config, checkpoint_path=checkpoints_dir)\n",
    "if wandb_active:\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ferdousi = copy.deepcopy(model)\n",
    "dataset.poet = 'ferdousi'\n",
    "run_name = 'ferdousi_fine_tune'\n",
    "if wandb_active:\n",
    "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
    "train(dataset, model_ferdousi, config, checkpoint_path=checkpoints_dir)\n",
    "if wandb_active:\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_hafez = copy.deepcopy(model)\n",
    "dataset.poet = 'hafez'\n",
    "run_name = 'hafez_fine_tune'\n",
    "if wandb_active:\n",
    "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
    "train(dataset, model_hafez, config, checkpoint_path=checkpoints_dir)\n",
    "if wandb_active:\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_moulavi = copy.deepcopy(model)\n",
    "dataset.poet = 'moulavi'\n",
    "run_name = 'moulavi_fine_tune'\n",
    "if wandb_active:\n",
    "    wandb.init(project=project_name, name=run_name, reinit=True)\n",
    "train(dataset, model_moulavi, config, checkpoint_path=checkpoints_dir)\n",
    "if wandb_active:\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(config, device)\n",
    "chechkpoint = torch.load('/content/drive/MyDrive/NLP Class/checkpoints/model_checkpoint_1654448961.97679.pt',\n",
    "                         map_location=torch.device('cpu'))\n",
    "print(chechkpoint['epoch'])\n",
    "model.load_state_dict(chechkpoint['model_state_dict'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vQI80qiTLb8",
    "outputId": "309bffc5-18d2-4b22-ed24-5810d6421102"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOM_ferdousi]\n",
      "توانا\n",
      "بود\n",
      "هر\n",
      "که\n",
      "جانند\n",
      "مستوی\n",
      "سنگیان\n",
      "پنداشت\n",
      "مالامالیم\n",
      "انگاشتیم\n",
      "مقراضی\n",
      "خضرم\n",
      "چفسیده\n",
      "التقا\n",
      "شهرهار\n",
      "خاکا\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(predict(dataset, model, text='[BOM_ferdousi] توانا بود هر که')))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rAGcmLZ2nNsJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e1ce9911-7781-4703-af4b-56e7894f9bc0"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "NLP-HW3-LSTM.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}